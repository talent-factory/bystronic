{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Datenimport und -export Tutorial\n",
    "\n",
    "**Python Grundkurs fÃ¼r Bystronic-Entwickler - Modul 06**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Lernziele\n",
    "\n",
    "In diesem Tutorial lernen Sie:\n",
    "- CSV-Dateien mit komplexen Strukturen zu importieren\n",
    "- Excel-Dateien mit mehreren ArbeitsblÃ¤ttern zu verarbeiten\n",
    "- JSON-Daten aus Dateien und APIs zu laden\n",
    "- Daten zu bereinigen und zu transformieren\n",
    "- Ergebnisse in verschiedenen Formaten zu exportieren\n",
    "\n",
    "## ğŸ“‹ Inhalt\n",
    "1. [CSV-Import: Bystronic Maschinendaten](#csv)\n",
    "2. [Excel-Verarbeitung](#excel) \n",
    "3. [JSON und APIs](#json)\n",
    "4. [Datenbereinigung](#cleaning)\n",
    "5. [Export und Speicherung](#export)\n",
    "6. [Praxis-Pipeline](#pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Alle Bibliotheken erfolgreich importiert!\n",
      "ğŸ“Š Pandas Version: 2.3.2\n",
      "ğŸ”¢ NumPy Version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Importiere notwendige Bibliotheken\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Konfiguration fÃ¼r bessere Darstellung\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Alle Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"ğŸ“Š Pandas Version: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. CSV-Import: Bystronic Maschinendaten {#csv}\n",
    "\n",
    "### ğŸ” Problem: Komplexe CSV-Struktur\n",
    "\n",
    "Die V084_Scope.csv Datei hat eine spezielle Struktur:\n",
    "- **Zeilen 1-6**: Metadaten (Name, Datei, Zeitstempel)\n",
    "- **Zeile 7**: Spaltennamen (nach \"Name\" Tag)\n",
    "- **Zeilen 8-21**: Weitere Metadaten (Kommentare, Datentypen, etc.)\n",
    "- **Ab Zeile 22**: Echte Messdaten\n",
    "- **Trennzeichen**: Tab (`\\t`)\n",
    "\n",
    "### ğŸ“ Schritt 1: Datei analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyse der Datei: ../../data/large/V084_Scope.csv\n",
      "================================================================================\n",
      " 1: Name\tDistance Control\n",
      " 2: File\tO:\\Messungen\\NDC\\MP4_T001\\V084_Scope.csv\n",
      " 3: Starttime of export\t133964386997165000\tDienstag, 8. Juli 2025\t09:58:19.716\n",
      " 4: Endtime of export\t133964388292505000\tDienstag, 8. Juli 2025\t10:00:29.250\n",
      " 5: \n",
      " 6: \n",
      " 7: Name\tDISTCTRL kp\tName\tDISTCTRL d_tv\tName\tDISTCTRL a_max\tName\tDISTCTRL a_max_int\tName\tDISTCTRL v_max\t...\n",
      " 8: SymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComm...\n",
      " 9: Data-Type\tREAL64\tData-Type\tREAL64\tData-Type\tINT32\tData-Type\tREAL64\tData-Type\tINT32\tData-Type\tREAL64\t...\n",
      "10: SampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t...\n",
      "11: VariableSize\t8\tVariableSize\t8\tVariableSize\t4\tVariableSize\t8\tVariableSize\t4\tVariableSize\t8\tVariableSi...\n",
      "12: SymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBase...\n",
      "13: IndexGroup\t1180416\tIndexGroup\t1180416\tIndexGroup\t1180416\tIndexGroup\t1180416\tIndexGroup\t1180416\tIndex...\n",
      "14: IndexOffset\t328147\tIndexOffset\t328158\tIndexOffset\t328020\tIndexOffset\t328023\tIndexOffset\t328019\tIndex...\n",
      "15: SymbolName\tDISTCTRL::kp\tSymbolName\tDISTCTRL::d_tv\tSymbolName\tDISTCTRL::a_max\tSymbolName\tDISTCTRL::a_...\n",
      "16: NetID\t192.168.100.1.1.1\tNetID\t192.168.100.1.1.1\tNetID\t192.168.100.1.1.1\tNetID\t192.168.100.1.1.1\tNetI...\n",
      "17: Port\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t851\tPort\t551\tPort\t551\tP...\n",
      "18: Offset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tO...\n",
      "19: ScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tSc...\n",
      "20: BitMask\t0xffffffffffffffff\tBitMask\t0xffffffffffffffff\tBitMask\t0xffffffffffffffff\tBitMask\t0xfffffffff...\n",
      "21: \n",
      "22: 0\t1\t0\t0\t0\t10000\t0\t100000000\t0\t50000\t0\t5000000\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t12\t0\t100\t0\t1\t0\t0...\n",
      "23: 1\t1\t1\t0\t1\t10000\t1\t100000000\t1\t50000\t1\t5000000\t1\t1\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t12\t1\t100\t1\t1\t1\t0...\n",
      "24: 2\t1\t2\t0\t2\t10000\t2\t100000000\t2\t50000\t2\t5000000\t2\t1\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t12\t2\t100\t2\t1\t2\t0...\n",
      "25: 3\t1\t3\t0\t3\t10000\t3\t100000000\t3\t50000\t3\t5000000\t3\t1\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t15\t3\t100\t3\t1\t3\t0...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pfad zur Bystronic CSV-Datei\n",
    "csv_file = \"../../data/large/V084_Scope.csv\"\n",
    "\n",
    "\n",
    "# Erste Zeilen der Datei anschauen\n",
    "def analyze_csv_structure(file_path, n_lines=25):\n",
    "    \"\"\"\n",
    "    Analysiert die Struktur einer CSV-Datei\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Analyse der Datei: {file_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n_lines:\n",
    "                break\n",
    "            # Zeige Zeilennummer und Inhalt (begrenzt)\n",
    "            content = (\n",
    "                line.strip()[:100] + \"...\" if len(line.strip()) > 100 else line.strip()\n",
    "            )\n",
    "            print(f\"{i + 1:2d}: {content}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# Analysiere die Struktur\n",
    "analyze_csv_structure(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Schritt 2: Intelligenter CSV-Parser\n",
    "\n",
    "Wir erstellen einen Parser, der:\n",
    "- Automatisch die Header-Zeile findet\n",
    "- Den Datenbereich identifiziert\n",
    "- Flexible Trennzeichen unterstÃ¼tzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bystronic_csv(\n",
    "    file_path, header_line=None, data_start_line=None, delimiter=\"\\t\", encoding=\"utf-8\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Flexibler Parser fÃ¼r Bystronic CSV-Dateien\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Pfad zur CSV-Datei\n",
    "    header_line : int, optional\n",
    "        Zeilennummer fÃ¼r Header (1-basiert, z.B. 7). Automatisch erkannt wenn None.\n",
    "    data_start_line : int, optional\n",
    "        Zeilennummer fÃ¼r Datenbeginn (1-basiert). Automatisch erkannt wenn None.\n",
    "    delimiter : str\n",
    "        Trennzeichen (Standard: Tab)\n",
    "    encoding : str\n",
    "        Encoding (Standard: UTF-8)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict: {\n",
    "        'metadata': dict,\n",
    "        'data': pd.DataFrame,\n",
    "        'info': dict\n",
    "    }\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“‚ Lade Datei: {file_path}\")\n",
    "\n",
    "    # Schritt 1: Datei einlesen\n",
    "    with open(file_path, encoding=encoding, errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Schritt 2: Metadaten extrahieren (erste 6 Zeilen)\n",
    "    metadata = {}\n",
    "    for i in range(min(6, len(lines))):\n",
    "        parts = lines[i].strip().split(delimiter, 1)\n",
    "        if len(parts) == 2:\n",
    "            key, value = parts\n",
    "            metadata[key] = value\n",
    "\n",
    "    # Schritt 3: Header-Zeile bestimmen\n",
    "    columns = []\n",
    "    if header_line is not None:\n",
    "        # Benutzer hat Header-Zeile explizit angegeben\n",
    "        header_row_index = header_line - 1  # Konvertiere zu 0-basiert\n",
    "        if header_row_index < len(lines):\n",
    "            header_content = lines[header_row_index].strip()\n",
    "            print(f\"ğŸ“‹ Verwende Header-Zeile {header_line}: {header_content[:100]}...\")\n",
    "\n",
    "            parts = header_content.split(delimiter)\n",
    "            # Extrahiere Spaltennamen (jede zweite Spalte nach \"Name\")\n",
    "            for j in range(1, len(parts), 2):\n",
    "                if j < len(parts) and parts[j].strip():\n",
    "                    columns.append(parts[j].strip())\n",
    "        else:\n",
    "            print(\n",
    "                f\"âš ï¸ Header-Zeile {header_line} existiert nicht (nur {len(lines)} Zeilen)\"\n",
    "            )\n",
    "    else:\n",
    "        # Automatische Header-Erkennung\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(\"Name\" + delimiter):\n",
    "                header_row_index = i\n",
    "                header_line = i + 1\n",
    "                parts = line.strip().split(delimiter)\n",
    "                # Extrahiere Spaltennamen\n",
    "                for j in range(1, len(parts), 2):\n",
    "                    if j < len(parts) and parts[j].strip():\n",
    "                        columns.append(parts[j].strip())\n",
    "                print(f\"ğŸ“‹ Header automatisch erkannt in Zeile {header_line}\")\n",
    "                break\n",
    "\n",
    "    # Schritt 4: Datenbereich bestimmen\n",
    "    if data_start_line is not None:\n",
    "        # Benutzer hat Datenstart explizit angegeben\n",
    "        data_start_row_index = data_start_line - 1\n",
    "        print(f\"ğŸ“Š Verwende Datenstart-Zeile {data_start_line}\")\n",
    "    else:\n",
    "        # Automatische Datenbereich-Erkennung\n",
    "        data_start_row_index = None\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i].strip()\n",
    "            if line and not any(\n",
    "                keyword in line\n",
    "                for keyword in [\n",
    "                    \"Name\",\n",
    "                    \"Data-Type\",\n",
    "                    \"SampleTime\",\n",
    "                    \"SymbolComment\",\n",
    "                    \"VariableSize\",\n",
    "                    \"SymbolBased\",\n",
    "                    \"IndexGroup\",\n",
    "                ]\n",
    "            ):\n",
    "                # PrÃ¼fe ob Zeile hauptsÃ¤chlich aus Zahlen besteht\n",
    "                parts = line.split(delimiter)\n",
    "                numeric_count = 0\n",
    "                for part in parts[: min(10, len(parts))]:  # PrÃ¼fe erste 10 Spalten\n",
    "                    try:\n",
    "                        float(part)\n",
    "                        numeric_count += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                if (\n",
    "                    numeric_count >= min(len(parts), 10) * 0.7\n",
    "                ):  # 70% der Werte sind Zahlen\n",
    "                    data_start_row_index = i\n",
    "                    data_start_line = i + 1\n",
    "                    print(\n",
    "                        f\"ğŸ“Š Datenbereich automatisch erkannt ab Zeile {data_start_line}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "    print(f\"ğŸ”¢ Anzahl Spalten gefunden: {len(columns)}\")\n",
    "    if columns:\n",
    "        print(\n",
    "            f\"ğŸ“ Beispiel-Spalten: {', '.join(columns[:5])}{'...' if len(columns) > 5 else ''}\"\n",
    "        )\n",
    "\n",
    "    # Schritt 5: DataFrame erstellen\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    if data_start_row_index is not None and columns:\n",
    "        try:\n",
    "            # Datenzeilen extrahieren\n",
    "            data_lines = lines[data_start_row_index:]\n",
    "\n",
    "            parsed_data = []\n",
    "            for line_num, line in enumerate(data_lines):\n",
    "                parts = line.strip().split(delimiter)\n",
    "                if len(parts) >= len(columns):\n",
    "                    parsed_data.append(parts[: len(columns)])\n",
    "\n",
    "                # Begrenzte Anzahl fÃ¼r Demo (kann entfernt werden fÃ¼r vollstÃ¤ndigen Import)\n",
    "                if len(parsed_data) >= 1000:  # Lade max. 1000 Zeilen fÃ¼r Demo\n",
    "                    print(\"â„¹ï¸ Demo-Modus: Nur erste 1000 Zeilen geladen\")\n",
    "                    break\n",
    "\n",
    "            if parsed_data:\n",
    "                df = pd.DataFrame(parsed_data, columns=columns)\n",
    "\n",
    "                # Datentyp-Konvertierung\n",
    "                print(\"ğŸ”„ Konvertiere Datentypen...\")\n",
    "                for col in df.columns:\n",
    "                    try:\n",
    "                        # Versuche numerische Konvertierung\n",
    "                        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                    except:\n",
    "                        # Behalte als String bei Fehlern\n",
    "                        pass\n",
    "\n",
    "                print(\n",
    "                    f\"âœ… DataFrame erstellt: {df.shape[0]:,} Zeilen Ã— {df.shape[1]} Spalten\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"âš ï¸ Keine gÃ¼ltigen Datenzeilen gefunden\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Fehler beim DataFrame-Erstellen: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"metadata\": metadata,\n",
    "        \"data\": df,\n",
    "        \"info\": {\n",
    "            \"header_line\": header_line,\n",
    "            \"data_start_line\": data_start_line,\n",
    "            \"total_lines\": len(lines),\n",
    "            \"columns\": columns,\n",
    "            \"parsing_success\": not df.empty,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Parser testen - Benutzer kann Header-Zeile explizit angeben\n",
    "print(\"ğŸ”§ Test 1: Automatische Erkennung\")\n",
    "result_auto = parse_bystronic_csv(csv_file)\n",
    "\n",
    "print(\"\\nğŸ”§ Test 2: Header-Zeile 7 explizit angeben\")\n",
    "result_manual = parse_bystronic_csv(csv_file, header_line=7, data_start_line=22)\n",
    "\n",
    "print(\"\\nğŸ“‹ Metadaten:\")\n",
    "for key, value in result_manual[\"metadata\"].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ“Š DataFrame Info:\")\n",
    "if not result_manual[\"data\"].empty:\n",
    "    print(result_manual[\"data\"].info())\n",
    "    print(\"\\nğŸ” Erste 5 Zeilen:\")\n",
    "    print(result_manual[\"data\"].head())\n",
    "else:\n",
    "    print(\"âŒ Kein DataFrame erstellt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Schritt 3: Daten erkunden und visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame fÃ¼r einfachere Verwendung (verwende die manuell geparste Version)\n",
    "df = result_manual[\"data\"]\n",
    "\n",
    "print(\"ğŸ” Erste 5 Zeilen:\")\n",
    "if not df.empty:\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nğŸ“ˆ Statistische Ãœbersicht:\")\n",
    "    display(df.describe())\n",
    "\n",
    "    # Interessante Spalten fÃ¼r Visualisierung finden\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nğŸ”¢ Numerische Spalten ({len(numeric_cols)}):\")\n",
    "    for i, col in enumerate(numeric_cols[:10]):  # Erste 10 anzeigen\n",
    "        print(f\"  {i + 1:2d}. {col}\")\n",
    "\n",
    "    if len(numeric_cols) >= 2:\n",
    "        # Beispiel-Visualisierung\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\"ğŸ”§ Bystronic Maschinendaten - Analyse\", fontsize=16)\n",
    "\n",
    "        # Plot 1: Zeitreihe der ersten numerischen Spalte\n",
    "        col1 = numeric_cols[0]\n",
    "        axes[0, 0].plot(df.index, df[col1], alpha=0.7)\n",
    "        axes[0, 0].set_title(f\"Zeitverlauf: {col1}\")\n",
    "        axes[0, 0].set_xlabel(\"Messung\")\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 2: Histogram\n",
    "        col2 = numeric_cols[1] if len(numeric_cols) > 1 else numeric_cols[0]\n",
    "        axes[0, 1].hist(df[col2].dropna(), bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "        axes[0, 1].set_title(f\"Verteilung: {col2}\")\n",
    "        axes[0, 1].set_xlabel(\"Wert\")\n",
    "        axes[0, 1].set_ylabel(\"HÃ¤ufigkeit\")\n",
    "\n",
    "        # Plot 3: Scatter Plot (falls genÃ¼gend Spalten)\n",
    "        if len(numeric_cols) >= 3:\n",
    "            col3 = numeric_cols[2]\n",
    "            axes[1, 0].scatter(df[col1], df[col3], alpha=0.6, s=20)\n",
    "            axes[1, 0].set_title(f\"Korrelation: {col1} vs {col3}\")\n",
    "            axes[1, 0].set_xlabel(col1)\n",
    "            axes[1, 0].set_ylabel(col3)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Boxplot der ersten 5 numerischen Spalten\n",
    "        sample_cols = numeric_cols[:5]\n",
    "        df_sample = df[sample_cols]\n",
    "        # Normalisiere Daten fÃ¼r bessere Darstellung\n",
    "        df_normalized = (df_sample - df_sample.mean()) / df_sample.std()\n",
    "        axes[1, 1].boxplot(\n",
    "            [df_normalized[col].dropna() for col in sample_cols],\n",
    "            labels=[col[:15] + \"...\" if len(col) > 15 else col for col in sample_cols],\n",
    "        )\n",
    "        axes[1, 1].set_title(\"Normalisierte Boxplots\")\n",
    "        axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"âœ… Visualisierung mit {len(sample_cols)} Parametern erstellt\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ Nicht genÃ¼gend numerische Spalten fÃ¼r erweiterte Visualisierung\")\n",
    "else:\n",
    "    print(\"âŒ Kein DataFrame verfÃ¼gbar fÃ¼r Analyse\")\n",
    "\n",
    "# Zeige Verwendungsbeispiele\n",
    "print(\"\\nğŸ’¡ Verwendungsbeispiele:\")\n",
    "print(\"# Automatische Erkennung:\")\n",
    "print(\"result = parse_bystronic_csv('V084_Scope.csv')\")\n",
    "print(\"\")\n",
    "print(\"# Header-Zeile 7, Daten ab Zeile 22:\")\n",
    "print(\n",
    "    \"result = parse_bystronic_csv('V084_Scope.csv', header_line=7, data_start_line=22)\"\n",
    ")\n",
    "print(\"\")\n",
    "print(\"# Nur Header-Zeile angeben:\")\n",
    "print(\"result = parse_bystronic_csv('V084_Scope.csv', header_line=7)\")\n",
    "print(\"\")\n",
    "print(\"# DataFrame zugreifen:\")\n",
    "print(\"df = result['data']\")\n",
    "print(\"metadata = result['metadata']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Excel-Verarbeitung {#excel}\n",
    "\n",
    "### ğŸ“ Erstelle Beispiel-Excel-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine Beispiel-Excel-Datei mit mehreren ArbeitsblÃ¤ttern\n",
    "def create_sample_excel(file_path):\n",
    "    \"\"\"\n",
    "    Erstellt eine Beispiel-Excel-Datei fÃ¼r Bystronic\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "        # Arbeitsblatt 1: Produktionsdaten\n",
    "        np.random.seed(42)\n",
    "        dates = pd.date_range(\"2024-01-01\", \"2024-12-31\", freq=\"D\")\n",
    "        production_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Datum\": dates,\n",
    "                \"Maschine_A_StÃ¼ck\": np.random.normal(1000, 100, len(dates)).astype(int),\n",
    "                \"Maschine_B_StÃ¼ck\": np.random.normal(800, 80, len(dates)).astype(int),\n",
    "                \"Maschine_C_StÃ¼ck\": np.random.normal(1200, 150, len(dates)).astype(int),\n",
    "                \"VerfÃ¼gbarkeit_A_%\": np.random.normal(85, 5, len(dates)),\n",
    "                \"VerfÃ¼gbarkeit_B_%\": np.random.normal(82, 6, len(dates)),\n",
    "                \"VerfÃ¼gbarkeit_C_%\": np.random.normal(88, 4, len(dates)),\n",
    "            }\n",
    "        )\n",
    "        production_data.to_excel(writer, sheet_name=\"Produktion\", index=False)\n",
    "\n",
    "        # Arbeitsblatt 2: QualitÃ¤tsdaten\n",
    "        quality_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Monat\": pd.date_range(\"2024-01-01\", \"2024-12-01\", freq=\"M\"),\n",
    "                \"Ausschuss_Rate_%\": np.random.normal(2.5, 0.5, 12),\n",
    "                \"Nacharbeit_Rate_%\": np.random.normal(1.2, 0.3, 12),\n",
    "                \"Kundenzufriedenheit\": np.random.normal(4.2, 0.2, 12),\n",
    "                \"Reklamationen_Anzahl\": np.random.poisson(5, 12),\n",
    "            }\n",
    "        )\n",
    "        quality_data.to_excel(writer, sheet_name=\"QualitÃ¤t\", index=False)\n",
    "\n",
    "        # Arbeitsblatt 3: Wartungsdaten\n",
    "        maintenance_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Maschine\": [\"A\", \"B\", \"C\"] * 50,\n",
    "                \"Wartungsdatum\": pd.date_range(\"2024-01-01\", periods=150, freq=\"3D\"),\n",
    "                \"Typ\": np.random.choice([\"PrÃ¤ventiv\", \"Korrektiv\", \"Inspektion\"], 150),\n",
    "                \"Dauer_h\": np.random.exponential(2, 150),\n",
    "                \"Kosten_EUR\": np.random.gamma(2, 500, 150),\n",
    "            }\n",
    "        )\n",
    "        maintenance_data.to_excel(writer, sheet_name=\"Wartung\", index=False)\n",
    "\n",
    "        # Arbeitsblatt 4: Konfiguration\n",
    "        config_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Parameter\": [\n",
    "                    \"Max_Geschwindigkeit\",\n",
    "                    \"Toleranz_X\",\n",
    "                    \"Toleranz_Y\",\n",
    "                    \"Laser_Power\",\n",
    "                    \"KÃ¼hlmittel_Temp\",\n",
    "                ],\n",
    "                \"Maschine_A\": [2500, 0.05, 0.05, 6000, 20],\n",
    "                \"Maschine_B\": [2200, 0.08, 0.06, 5500, 22],\n",
    "                \"Maschine_C\": [2800, 0.04, 0.04, 6500, 18],\n",
    "                \"Einheit\": [\"mm/min\", \"mm\", \"mm\", \"W\", \"Â°C\"],\n",
    "            }\n",
    "        )\n",
    "        config_data.to_excel(writer, sheet_name=\"Konfiguration\", index=False)\n",
    "\n",
    "    print(f\"âœ… Excel-Datei erstellt: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# Excel-Datei erstellen\n",
    "excel_file = \"../../data/bystronic_production_data.xlsx\"\n",
    "create_sample_excel(excel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Excel-Datei laden und analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel-Datei mit allen ArbeitsblÃ¤ttern laden\n",
    "def load_excel_comprehensive(file_path):\n",
    "    \"\"\"\n",
    "    LÃ¤dt eine Excel-Datei mit allen ArbeitsblÃ¤ttern\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“‚ Lade Excel-Datei: {file_path}\")\n",
    "\n",
    "    # Alle ArbeitsblÃ¤tter laden\n",
    "    excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    print(f\"ğŸ“‹ ArbeitsblÃ¤tter gefunden: {list(excel_data.keys())}\")\n",
    "\n",
    "    for sheet_name, df in excel_data.items():\n",
    "        print(f\"\\nğŸ“Š {sheet_name}: {df.shape[0]} Zeilen Ã— {df.shape[1]} Spalten\")\n",
    "        print(\n",
    "            f\"   Spalten: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\"\n",
    "        )\n",
    "\n",
    "    return excel_data\n",
    "\n",
    "\n",
    "# Excel laden\n",
    "excel_data = load_excel_comprehensive(excel_file)\n",
    "\n",
    "# Einzelne ArbeitsblÃ¤tter analysieren\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“‹ PRODUKTIONSDATEN ANALYSE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "production_df = excel_data[\"Produktion\"]\n",
    "display(production_df.head())\n",
    "\n",
    "print(\"\\nğŸ“ˆ Produktionsstatistiken:\")\n",
    "production_stats = production_df.select_dtypes(include=[np.number]).describe()\n",
    "display(production_stats)\n",
    "\n",
    "# Visualisierung der Produktionsdaten\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"ğŸ­ Bystronic Produktionsanalyse\", fontsize=16)\n",
    "\n",
    "# Plot 1: Produktionsmenge Ã¼ber Zeit\n",
    "production_df.plot(\n",
    "    x=\"Datum\",\n",
    "    y=[\"Maschine_A_StÃ¼ck\", \"Maschine_B_StÃ¼ck\", \"Maschine_C_StÃ¼ck\"],\n",
    "    ax=axes[0, 0],\n",
    "    title=\"Produktionsmenge pro Tag\",\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Datum\")\n",
    "axes[0, 0].set_ylabel(\"StÃ¼ck pro Tag\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: VerfÃ¼gbarkeit Ã¼ber Zeit\n",
    "production_df.plot(\n",
    "    x=\"Datum\",\n",
    "    y=[\"VerfÃ¼gbarkeit_A_%\", \"VerfÃ¼gbarkeit_B_%\", \"VerfÃ¼gbarkeit_C_%\"],\n",
    "    ax=axes[0, 1],\n",
    "    title=\"MaschinenverfÃ¼gbarkeit\",\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Datum\")\n",
    "axes[0, 1].set_ylabel(\"VerfÃ¼gbarkeit %\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Korrelation Produktion vs VerfÃ¼gbarkeit\n",
    "axes[1, 0].scatter(\n",
    "    production_df[\"VerfÃ¼gbarkeit_A_%\"], production_df[\"Maschine_A_StÃ¼ck\"], alpha=0.6\n",
    ")\n",
    "axes[1, 0].set_title(\"Korrelation: VerfÃ¼gbarkeit vs Produktion (Maschine A)\")\n",
    "axes[1, 0].set_xlabel(\"VerfÃ¼gbarkeit %\")\n",
    "axes[1, 0].set_ylabel(\"Produktion StÃ¼ck/Tag\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Monatliche Zusammenfassung\n",
    "monthly_prod = production_df.copy()\n",
    "monthly_prod[\"Monat\"] = monthly_prod[\"Datum\"].dt.to_period(\"M\")\n",
    "monthly_summary = monthly_prod.groupby(\"Monat\")[\n",
    "    [\"Maschine_A_StÃ¼ck\", \"Maschine_B_StÃ¼ck\", \"Maschine_C_StÃ¼ck\"]\n",
    "].sum()\n",
    "monthly_summary.plot(kind=\"bar\", ax=axes[1, 1], title=\"Monatsproduktion\")\n",
    "axes[1, 1].set_xlabel(\"Monat\")\n",
    "axes[1, 1].set_ylabel(\"Gesamt StÃ¼ck\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. JSON und APIs {#json}\n",
    "\n",
    "### ğŸ“ JSON-Daten verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel JSON fÃ¼r IoT-Sensordaten erstellen\n",
    "def create_sample_json(file_path):\n",
    "    \"\"\"\n",
    "    Erstellt eine Beispiel-JSON-Datei mit IoT-Sensordaten\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    sensor_data = {\n",
    "        \"metadata\": {\n",
    "            \"system\": \"Bystronic IoT Platform\",\n",
    "            \"version\": \"2.1.0\",\n",
    "            \"timestamp\": \"2024-01-15T08:30:00Z\",\n",
    "            \"location\": \"Werk_NiederÃ¶nz\",\n",
    "        },\n",
    "        \"sensors\": [\n",
    "            {\n",
    "                \"id\": \"TEMP_001\",\n",
    "                \"type\": \"temperature\",\n",
    "                \"location\": \"Laser_Cutting_1\",\n",
    "                \"unit\": \"celsius\",\n",
    "                \"readings\": [\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:30:00Z\",\n",
    "                        \"value\": 22.5,\n",
    "                        \"status\": \"ok\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:31:00Z\",\n",
    "                        \"value\": 22.8,\n",
    "                        \"status\": \"ok\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:32:00Z\",\n",
    "                        \"value\": 23.1,\n",
    "                        \"status\": \"ok\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:33:00Z\",\n",
    "                        \"value\": 23.4,\n",
    "                        \"status\": \"warning\",\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"VIBR_001\",\n",
    "                \"type\": \"vibration\",\n",
    "                \"location\": \"Laser_Cutting_1\",\n",
    "                \"unit\": \"mm/s\",\n",
    "                \"readings\": [\n",
    "                    {\"timestamp\": \"2024-01-15T08:30:00Z\", \"value\": 1.2, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:31:00Z\", \"value\": 1.4, \"status\": \"ok\"},\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:32:00Z\",\n",
    "                        \"value\": 2.1,\n",
    "                        \"status\": \"warning\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:33:00Z\",\n",
    "                        \"value\": 2.8,\n",
    "                        \"status\": \"error\",\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"POWER_001\",\n",
    "                \"type\": \"power\",\n",
    "                \"location\": \"Laser_Cutting_1\",\n",
    "                \"unit\": \"kW\",\n",
    "                \"readings\": [\n",
    "                    {\"timestamp\": \"2024-01-15T08:30:00Z\", \"value\": 6.2, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:31:00Z\", \"value\": 6.0, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:32:00Z\", \"value\": 5.8, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:33:00Z\", \"value\": 5.5, \"status\": \"ok\"},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        \"alerts\": [\n",
    "            {\n",
    "                \"id\": \"ALT_001\",\n",
    "                \"timestamp\": \"2024-01-15T08:33:00Z\",\n",
    "                \"severity\": \"high\",\n",
    "                \"sensor_id\": \"VIBR_001\",\n",
    "                \"message\": \"Vibration level exceeds threshold\",\n",
    "                \"threshold\": 2.5,\n",
    "                \"actual_value\": 2.8,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sensor_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… JSON-Datei erstellt: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# JSON-Datei erstellen\n",
    "json_file = \"../../data/sensor_data.json\"\n",
    "create_sample_json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON-Daten laden und verarbeiten\n",
    "def process_sensor_json(file_path):\n",
    "    \"\"\"\n",
    "    Verarbeitet JSON-Sensordaten und erstellt DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“‚ Lade JSON-Datei: {file_path}\")\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"ğŸ­ System: {data['metadata']['system']}\")\n",
    "    print(f\"ğŸ“ Standort: {data['metadata']['location']}\")\n",
    "    print(f\"ğŸ“Š Sensoren: {len(data['sensors'])}\")\n",
    "    print(f\"âš ï¸ Alerts: {len(data['alerts'])}\")\n",
    "\n",
    "    # Sensordaten in DataFrame umwandeln\n",
    "    sensor_readings = []\n",
    "\n",
    "    for sensor in data[\"sensors\"]:\n",
    "        sensor_id = sensor[\"id\"]\n",
    "        sensor_type = sensor[\"type\"]\n",
    "        location = sensor[\"location\"]\n",
    "        unit = sensor[\"unit\"]\n",
    "\n",
    "        for reading in sensor[\"readings\"]:\n",
    "            sensor_readings.append(\n",
    "                {\n",
    "                    \"sensor_id\": sensor_id,\n",
    "                    \"type\": sensor_type,\n",
    "                    \"location\": location,\n",
    "                    \"unit\": unit,\n",
    "                    \"timestamp\": pd.to_datetime(reading[\"timestamp\"]),\n",
    "                    \"value\": reading[\"value\"],\n",
    "                    \"status\": reading[\"status\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_sensors = pd.DataFrame(sensor_readings)\n",
    "\n",
    "    # Alerts DataFrame\n",
    "    df_alerts = pd.DataFrame(data[\"alerts\"])\n",
    "    if not df_alerts.empty:\n",
    "        df_alerts[\"timestamp\"] = pd.to_datetime(df_alerts[\"timestamp\"])\n",
    "\n",
    "    return df_sensors, df_alerts, data[\"metadata\"]\n",
    "\n",
    "\n",
    "# JSON verarbeiten\n",
    "df_sensors, df_alerts, metadata = process_sensor_json(json_file)\n",
    "\n",
    "print(\"\\nğŸ“Š Sensordaten:\")\n",
    "display(df_sensors)\n",
    "\n",
    "print(\"\\nâš ï¸ Alerts:\")\n",
    "display(df_alerts)\n",
    "\n",
    "# Visualisierung der Sensordaten\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"ğŸ”§ IoT Sensordaten Analysis\", fontsize=16)\n",
    "\n",
    "# Plot fÃ¼r jeden Sensortyp\n",
    "sensor_types = df_sensors[\"type\"].unique()\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\"]\n",
    "\n",
    "for i, sensor_type in enumerate(sensor_types[:4]):\n",
    "    row, col = i // 2, i % 2\n",
    "    sensor_data = df_sensors[df_sensors[\"type\"] == sensor_type]\n",
    "\n",
    "    axes[row, col].plot(\n",
    "        sensor_data[\"timestamp\"],\n",
    "        sensor_data[\"value\"],\n",
    "        color=colors[i],\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        markersize=6,\n",
    "    )\n",
    "    axes[row, col].set_title(f\"{sensor_type.upper()} Verlauf\")\n",
    "    axes[row, col].set_xlabel(\"Zeit\")\n",
    "    axes[row, col].set_ylabel(f\"{sensor_data.iloc[0]['unit']}\")\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "    # Status-basierte Farbcodierung\n",
    "    for status in [\"ok\", \"warning\", \"error\"]:\n",
    "        status_data = sensor_data[sensor_data[\"status\"] == status]\n",
    "        if not status_data.empty:\n",
    "            color_map = {\"ok\": \"green\", \"warning\": \"orange\", \"error\": \"red\"}\n",
    "            axes[row, col].scatter(\n",
    "                status_data[\"timestamp\"],\n",
    "                status_data[\"value\"],\n",
    "                c=color_map[status],\n",
    "                s=100,\n",
    "                alpha=0.7,\n",
    "                label=status,\n",
    "                edgecolors=\"black\",\n",
    "                linewidth=1,\n",
    "            )\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ API-Daten simulieren (Web Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuliere API-Aufruf (da wir keine echte API haben)\n",
    "def simulate_api_call(api_type=\"production\"):\n",
    "    \"\"\"\n",
    "    Simuliert verschiedene API-Aufrufe fÃ¼r Bystronic-Systeme\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    print(f\"ğŸŒ Simuliere API-Aufruf: {api_type}\")\n",
    "    time.sleep(1)  # Simuliere Netzwerk-Latenz\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if api_type == \"production\":\n",
    "        # Produktions-API\n",
    "        data = {\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"data\": {\n",
    "                \"machines\": [\n",
    "                    {\n",
    "                        \"id\": \"LC001\",\n",
    "                        \"name\": \"Laser_Cutting_1\",\n",
    "                        \"status\": \"running\",\n",
    "                        \"current_job\": \"JOB_2024_001\",\n",
    "                        \"parts_produced_today\": np.random.randint(800, 1200),\n",
    "                        \"efficiency\": np.random.uniform(85, 95),\n",
    "                        \"temperature\": np.random.uniform(20, 25),\n",
    "                    },\n",
    "                    {\n",
    "                        \"id\": \"LC002\",\n",
    "                        \"name\": \"Laser_Cutting_2\",\n",
    "                        \"status\": \"maintenance\",\n",
    "                        \"current_job\": None,\n",
    "                        \"parts_produced_today\": 0,\n",
    "                        \"efficiency\": 0,\n",
    "                        \"temperature\": np.random.uniform(18, 22),\n",
    "                    },\n",
    "                    {\n",
    "                        \"id\": \"BR001\",\n",
    "                        \"name\": \"Press_Brake_1\",\n",
    "                        \"status\": \"running\",\n",
    "                        \"current_job\": \"JOB_2024_002\",\n",
    "                        \"parts_produced_today\": np.random.randint(600, 900),\n",
    "                        \"efficiency\": np.random.uniform(80, 90),\n",
    "                        \"temperature\": np.random.uniform(22, 28),\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "\n",
    "    elif api_type == \"quality\":\n",
    "        # QualitÃ¤ts-API\n",
    "        data = {\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"data\": {\n",
    "                \"daily_summary\": {\n",
    "                    \"total_parts\": np.random.randint(2000, 3000),\n",
    "                    \"defective_parts\": np.random.randint(10, 50),\n",
    "                    \"defect_rate\": np.random.uniform(0.5, 2.5),\n",
    "                    \"rework_rate\": np.random.uniform(0.2, 1.0),\n",
    "                },\n",
    "                \"defect_types\": [\n",
    "                    {\"type\": \"dimensional\", \"count\": np.random.randint(5, 20)},\n",
    "                    {\"type\": \"surface\", \"count\": np.random.randint(2, 15)},\n",
    "                    {\"type\": \"material\", \"count\": np.random.randint(1, 8)},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # Allgemeine API\n",
    "        data = {\"status\": \"error\", \"message\": \"Unknown API endpoint\"}\n",
    "\n",
    "    print(f\"âœ… API Response: {data['status']}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# API-Aufrufe testen\n",
    "production_data = simulate_api_call(\"production\")\n",
    "quality_data = simulate_api_call(\"quality\")\n",
    "\n",
    "print(\"\\nğŸ“Š Produktions-API Daten:\")\n",
    "production_df = pd.json_normalize(production_data[\"data\"][\"machines\"])\n",
    "display(production_df)\n",
    "\n",
    "print(\"\\nğŸ” QualitÃ¤ts-API Daten:\")\n",
    "quality_summary = quality_data[\"data\"][\"daily_summary\"]\n",
    "defects_df = pd.DataFrame(quality_data[\"data\"][\"defect_types\"])\n",
    "\n",
    "print(f\"Tagesproduktion: {quality_summary['total_parts']} Teile\")\n",
    "print(f\"Ausschussrate: {quality_summary['defect_rate']:.2f}%\")\n",
    "print(\"\\nFehlertypen:\")\n",
    "display(defects_df)\n",
    "\n",
    "# Visualisierung der API-Daten\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"ğŸ”„ Live API-Daten Dashboard\", fontsize=16)\n",
    "\n",
    "# Plot 1: Maschineneffizienz\n",
    "running_machines = production_df[production_df[\"status\"] == \"running\"]\n",
    "if not running_machines.empty:\n",
    "    bars1 = axes[0].bar(\n",
    "        running_machines[\"name\"],\n",
    "        running_machines[\"efficiency\"],\n",
    "        color=[\n",
    "            \"green\" if x > 90 else \"orange\" if x > 80 else \"red\"\n",
    "            for x in running_machines[\"efficiency\"]\n",
    "        ],\n",
    "    )\n",
    "    axes[0].set_title(\"Maschineneffizienz (%)\")\n",
    "    axes[0].set_ylabel(\"Effizienz %\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Tagesproduktion\n",
    "all_machines = production_df[production_df[\"parts_produced_today\"] > 0]\n",
    "if not all_machines.empty:\n",
    "    bars2 = axes[1].bar(\n",
    "        all_machines[\"name\"],\n",
    "        all_machines[\"parts_produced_today\"],\n",
    "        color=\"steelblue\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[1].set_title(\"Tagesproduktion (Teile)\")\n",
    "    axes[1].set_ylabel(\"Anzahl Teile\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Fehlertypen\n",
    "if not defects_df.empty:\n",
    "    colors = [\"lightcoral\", \"lightsalmon\", \"lightblue\"]\n",
    "    bars3 = axes[2].pie(\n",
    "        defects_df[\"count\"],\n",
    "        labels=defects_df[\"type\"],\n",
    "        colors=colors,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "    )\n",
    "    axes[2].set_title(\"Fehlerverteilung\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Datenbereinigung {#cleaning}\n",
    "\n",
    "### ğŸ“ DatenqualitÃ¤t prÃ¼fen und verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen fÃ¼r Datenbereinigung\n",
    "def analyze_data_quality(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Analysiert die DatenqualitÃ¤t eines DataFrames\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” DatenqualitÃ¤tsanalyse: {name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"ğŸ“Š Shape: {df.shape[0]} Zeilen Ã— {df.shape[1]} Spalten\")\n",
    "    print(f\"ğŸ’¾ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Fehlende Werte\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nâŒ Fehlende Werte:\")\n",
    "        missing_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"Spalte\": missing_data.index,\n",
    "                \"Fehlend\": missing_data.values,\n",
    "                \"Prozent\": missing_percent.values,\n",
    "            }\n",
    "        )\n",
    "        missing_summary = missing_summary[missing_summary[\"Fehlend\"] > 0].sort_values(\n",
    "            \"Fehlend\", ascending=False\n",
    "        )\n",
    "        display(missing_summary)\n",
    "    else:\n",
    "        print(\"\\nâœ… Keine fehlenden Werte gefunden\")\n",
    "\n",
    "    # Datentypen\n",
    "    print(\"\\nğŸ“ˆ Datentypen:\")\n",
    "    dtype_summary = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_summary.items():\n",
    "        print(f\"  {dtype}: {count} Spalten\")\n",
    "\n",
    "    # Duplikate\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\nâš ï¸ Duplikate: {duplicates} Zeilen ({duplicates / len(df) * 100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Keine Duplikate gefunden\")\n",
    "\n",
    "    # Numerische Spalten analysieren\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nğŸ”¢ Numerische Analyse ({len(numeric_cols)} Spalten):\")\n",
    "        for col in numeric_cols[:5]:  # Erste 5 Spalten\n",
    "            series = df[col].dropna()\n",
    "            if len(series) > 0:\n",
    "                outliers = detect_outliers_iqr(series)\n",
    "                print(\n",
    "                    f\"  {col}: Min={series.min():.2f}, Max={series.max():.2f}, Outliers={len(outliers)}\"\n",
    "                )\n",
    "\n",
    "    return {\n",
    "        \"shape\": df.shape,\n",
    "        \"missing_values\": missing_data.sum(),\n",
    "        \"duplicates\": duplicates,\n",
    "        \"memory_mb\": df.memory_usage(deep=True).sum() / 1024**2,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(series, k=1.5):\n",
    "    \"\"\"\n",
    "    Erkennt AusreiÃŸer mit der IQR-Methode\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "# Analysiere verschiedene Datasets\n",
    "if not result[\"data\"].empty:\n",
    "    bystronic_quality = analyze_data_quality(result[\"data\"], \"Bystronic CSV\")\n",
    "\n",
    "production_quality = analyze_data_quality(production_df, \"Produktionsdaten\")\n",
    "sensors_quality = analyze_data_quality(df_sensors, \"Sensordaten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenbereinigung durchfÃ¼hren\n",
    "def clean_production_data(df):\n",
    "    \"\"\"\n",
    "    Bereinigt Produktionsdaten\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ Starte Datenbereinigung...\")\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Duplikate entfernen\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    removed_duplicates = initial_rows - len(df_clean)\n",
    "    if removed_duplicates > 0:\n",
    "        print(f\"âœ… {removed_duplicates} Duplikate entfernt\")\n",
    "\n",
    "    # 2. Unrealistische Werte korrigieren\n",
    "    # VerfÃ¼gbarkeit sollte zwischen 0-100% liegen\n",
    "    availability_cols = [col for col in df_clean.columns if \"VerfÃ¼gbarkeit\" in col]\n",
    "    for col in availability_cols:\n",
    "        # Werte > 100% auf 100% setzen\n",
    "        outliers_high = df_clean[col] > 100\n",
    "        if outliers_high.sum() > 0:\n",
    "            df_clean.loc[outliers_high, col] = 100\n",
    "            print(f\"âœ… {outliers_high.sum()} hohe AusreiÃŸer in {col} korrigiert\")\n",
    "\n",
    "        # Negative Werte auf 0 setzen\n",
    "        negative = df_clean[col] < 0\n",
    "        if negative.sum() > 0:\n",
    "            df_clean.loc[negative, col] = 0\n",
    "            print(f\"âœ… {negative.sum()} negative Werte in {col} korrigiert\")\n",
    "\n",
    "    # 3. Produktionswerte kÃ¶nnen nicht negativ sein\n",
    "    production_cols = [col for col in df_clean.columns if \"StÃ¼ck\" in col]\n",
    "    for col in production_cols:\n",
    "        negative = df_clean[col] < 0\n",
    "        if negative.sum() > 0:\n",
    "            df_clean.loc[negative, col] = 0\n",
    "            print(f\"âœ… {negative.sum()} negative Produktionswerte in {col} korrigiert\")\n",
    "\n",
    "        # Extreme AusreiÃŸer (> 99.9% Quantil) glÃ¤tten\n",
    "        threshold = df_clean[col].quantile(0.999)\n",
    "        extreme_outliers = df_clean[col] > threshold\n",
    "        if extreme_outliers.sum() > 0:\n",
    "            df_clean.loc[extreme_outliers, col] = threshold\n",
    "            print(f\"âœ… {extreme_outliers.sum()} extreme AusreiÃŸer in {col} geglÃ¤ttet\")\n",
    "\n",
    "    # 4. Datum validieren\n",
    "    if \"Datum\" in df_clean.columns:\n",
    "        # Sicherstellen, dass Datum im erwarteten Bereich liegt\n",
    "        min_date = pd.Timestamp(\"2020-01-01\")\n",
    "        max_date = pd.Timestamp(\"2030-12-31\")\n",
    "\n",
    "        invalid_dates = (df_clean[\"Datum\"] < min_date) | (df_clean[\"Datum\"] > max_date)\n",
    "        if invalid_dates.sum() > 0:\n",
    "            print(f\"âš ï¸ {invalid_dates.sum()} ungÃ¼ltige Datumswerte gefunden\")\n",
    "            df_clean = df_clean[~invalid_dates]\n",
    "\n",
    "    print(f\"ğŸ‰ Bereinigung abgeschlossen: {len(df_clean)} Zeilen verbleiben\")\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Produktionsdaten bereinigen\n",
    "production_clean = clean_production_data(production_df)\n",
    "\n",
    "# Vorher/Nachher Vergleich\n",
    "print(\"\\nğŸ“Š Bereinigung - Vorher/Nachher:\")\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Vorher\": [\n",
    "            len(production_df),\n",
    "            production_df.isnull().sum().sum(),\n",
    "            production_df.duplicated().sum(),\n",
    "        ],\n",
    "        \"Nachher\": [\n",
    "            len(production_clean),\n",
    "            production_clean.isnull().sum().sum(),\n",
    "            production_clean.duplicated().sum(),\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Zeilen\", \"Fehlende Werte\", \"Duplikate\"],\n",
    ")\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Visualisierung der Bereinigung\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"ğŸ§¹ Datenbereinigung: Vorher vs. Nachher\", fontsize=16)\n",
    "\n",
    "# Beispiel-Visualisierung mit Maschine A\n",
    "col_prod = \"Maschine_A_StÃ¼ck\"\n",
    "col_avail = \"VerfÃ¼gbarkeit_A_%\"\n",
    "\n",
    "# Vorher - Produktion\n",
    "axes[0, 0].hist(\n",
    "    production_df[col_prod], bins=30, alpha=0.7, color=\"red\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0, 0].set_title(f\"Vorher: {col_prod}\")\n",
    "axes[0, 0].set_xlabel(\"StÃ¼ck pro Tag\")\n",
    "axes[0, 0].set_ylabel(\"HÃ¤ufigkeit\")\n",
    "\n",
    "# Nachher - Produktion\n",
    "axes[0, 1].hist(\n",
    "    production_clean[col_prod], bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0, 1].set_title(f\"Nachher: {col_prod}\")\n",
    "axes[0, 1].set_xlabel(\"StÃ¼ck pro Tag\")\n",
    "axes[0, 1].set_ylabel(\"HÃ¤ufigkeit\")\n",
    "\n",
    "# Vorher - VerfÃ¼gbarkeit\n",
    "axes[1, 0].hist(\n",
    "    production_df[col_avail], bins=30, alpha=0.7, color=\"red\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 0].set_title(f\"Vorher: {col_avail}\")\n",
    "axes[1, 0].set_xlabel(\"VerfÃ¼gbarkeit %\")\n",
    "axes[1, 0].set_ylabel(\"HÃ¤ufigkeit\")\n",
    "\n",
    "# Nachher - VerfÃ¼gbarkeit\n",
    "axes[1, 1].hist(\n",
    "    production_clean[col_avail], bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 1].set_title(f\"Nachher: {col_avail}\")\n",
    "axes[1, 1].set_xlabel(\"VerfÃ¼gbarkeit %\")\n",
    "axes[1, 1].set_ylabel(\"HÃ¤ufigkeit\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Export und Speicherung {#export}\n",
    "\n",
    "### ğŸ“ Daten in verschiedenen Formaten exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export-Funktionen\n",
    "def export_data_comprehensive(df, base_filename, include_formats=None):\n",
    "    \"\"\"\n",
    "    Exportiert DataFrame in verschiedene Formate\n",
    "    \"\"\"\n",
    "    if include_formats is None:\n",
    "        include_formats = [\"csv\", \"excel\", \"json\", \"parquet\"]\n",
    "\n",
    "    print(f\"ğŸ’¾ Exportiere {df.shape[0]} Zeilen in {len(include_formats)} Formaten...\")\n",
    "    exported_files = []\n",
    "\n",
    "    # CSV Export\n",
    "    if \"csv\" in include_formats:\n",
    "        csv_file = f\"../../data/{base_filename}.csv\"\n",
    "        df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "        file_size = Path(csv_file).stat().st_size / 1024\n",
    "        exported_files.append(\n",
    "            {\"Format\": \"CSV\", \"Datei\": csv_file, \"GrÃ¶ÃŸe_KB\": f\"{file_size:.1f}\"}\n",
    "        )\n",
    "        print(f\"âœ… CSV: {csv_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "    # Excel Export\n",
    "    if \"excel\" in include_formats:\n",
    "        excel_file = f\"../../data/{base_filename}.xlsx\"\n",
    "        with pd.ExcelWriter(excel_file, engine=\"openpyxl\") as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Daten\", index=False)\n",
    "\n",
    "            # ZusÃ¤tzliches Statistik-Blatt\n",
    "            if not df.select_dtypes(include=[np.number]).empty:\n",
    "                stats = df.select_dtypes(include=[np.number]).describe()\n",
    "                stats.to_excel(writer, sheet_name=\"Statistiken\")\n",
    "\n",
    "        file_size = Path(excel_file).stat().st_size / 1024\n",
    "        exported_files.append(\n",
    "            {\"Format\": \"Excel\", \"Datei\": excel_file, \"GrÃ¶ÃŸe_KB\": f\"{file_size:.1f}\"}\n",
    "        )\n",
    "        print(f\"âœ… Excel: {excel_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "    # JSON Export\n",
    "    if \"json\" in include_formats:\n",
    "        json_file = f\"../../data/{base_filename}.json\"\n",
    "        # DataFrame zu Records konvertieren fÃ¼r bessere JSON-Struktur\n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                \"exported_at\": pd.Timestamp.now().isoformat(),\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": len(df.columns),\n",
    "                \"source\": \"Bystronic Data Pipeline\",\n",
    "            },\n",
    "            \"data\": df.to_dict(\"records\"),\n",
    "        }\n",
    "\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, default=str, ensure_ascii=False)\n",
    "\n",
    "        file_size = Path(json_file).stat().st_size / 1024\n",
    "        exported_files.append(\n",
    "            {\"Format\": \"JSON\", \"Datei\": json_file, \"GrÃ¶ÃŸe_KB\": f\"{file_size:.1f}\"}\n",
    "        )\n",
    "        print(f\"âœ… JSON: {json_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "    # Parquet Export (effizienter fÃ¼r groÃŸe DatensÃ¤tze)\n",
    "    if \"parquet\" in include_formats:\n",
    "        try:\n",
    "            parquet_file = f\"../../data/{base_filename}.parquet\"\n",
    "            df.to_parquet(parquet_file, index=False, compression=\"snappy\")\n",
    "            file_size = Path(parquet_file).stat().st_size / 1024\n",
    "            exported_files.append(\n",
    "                {\n",
    "                    \"Format\": \"Parquet\",\n",
    "                    \"Datei\": parquet_file,\n",
    "                    \"GrÃ¶ÃŸe_KB\": f\"{file_size:.1f}\",\n",
    "                }\n",
    "            )\n",
    "            print(f\"âœ… Parquet: {parquet_file} ({file_size:.1f} KB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Parquet Export fehlgeschlagen: {e}\")\n",
    "\n",
    "    # Zusammenfassung\n",
    "    summary_df = pd.DataFrame(exported_files)\n",
    "    print(\"\\nğŸ“‹ Export-Zusammenfassung:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    return exported_files\n",
    "\n",
    "\n",
    "# Bereinigte Produktionsdaten exportieren\n",
    "production_exports = export_data_comprehensive(\n",
    "    production_clean, \"bystronic_production_clean\"\n",
    ")\n",
    "\n",
    "# Sensordaten exportieren\n",
    "sensor_exports = export_data_comprehensive(\n",
    "    df_sensors, \"bystronic_sensors\", [\"csv\", \"json\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nğŸ‰ Gesamt exportiert: {len(production_exports) + len(sensor_exports)} Dateien\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Praxis-Pipeline {#pipeline}\n",
    "\n",
    "### ğŸ“ VollstÃ¤ndige Datenverarbeitungs-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VollstÃ¤ndige Datenverarbeitungs-Pipeline\n",
    "class BystronicDataPipeline:\n",
    "    \"\"\"\n",
    "    VollstÃ¤ndige Datenverarbeitungs-Pipeline fÃ¼r Bystronic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or self._default_config()\n",
    "        self.processed_data = {}\n",
    "        self.metadata = {}\n",
    "        self.log = []\n",
    "\n",
    "    def _default_config(self):\n",
    "        return {\n",
    "            \"csv_delimiter\": \"\\t\",\n",
    "            \"csv_encoding\": \"utf-8\",\n",
    "            \"clean_data\": True,\n",
    "            \"export_formats\": [\"csv\", \"excel\", \"json\"],\n",
    "            \"output_dir\": \"../../data/pipeline_output/\",\n",
    "        }\n",
    "\n",
    "    def log_message(self, message, level=\"INFO\"):\n",
    "        \"\"\"Logging fÃ¼r Pipeline\"\"\"\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {level}: {message}\"\n",
    "        self.log.append(log_entry)\n",
    "        print(log_entry)\n",
    "\n",
    "    def import_csv(self, file_path, data_name):\n",
    "        \"\"\"CSV Import mit intelligenter Struktur-Erkennung\"\"\"\n",
    "        self.log_message(f\"Importiere CSV: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            result = parse_bystronic_csv(\n",
    "                file_path,\n",
    "                delimiter=self.config[\"csv_delimiter\"],\n",
    "                encoding=self.config[\"csv_encoding\"],\n",
    "            )\n",
    "\n",
    "            self.processed_data[data_name] = result[\"data\"]\n",
    "            self.metadata[data_name] = result[\"metadata\"]\n",
    "\n",
    "            self.log_message(f\"CSV Import erfolgreich: {result['data'].shape}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"CSV Import fehlgeschlagen: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def import_excel(self, file_path, data_name):\n",
    "        \"\"\"Excel Import\"\"\"\n",
    "        self.log_message(f\"Importiere Excel: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "            for sheet_name, df in excel_data.items():\n",
    "                full_name = f\"{data_name}_{sheet_name}\"\n",
    "                self.processed_data[full_name] = df\n",
    "                self.log_message(f\"Arbeitsblatt '{sheet_name}': {df.shape}\")\n",
    "\n",
    "            self.log_message(\n",
    "                f\"Excel Import erfolgreich: {len(excel_data)} ArbeitsblÃ¤tter\"\n",
    "            )\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"Excel Import fehlgeschlagen: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def import_api(self, api_type, data_name):\n",
    "        \"\"\"API Import (simuliert)\"\"\"\n",
    "        self.log_message(f\"Importiere API-Daten: {api_type}\")\n",
    "\n",
    "        try:\n",
    "            api_data = simulate_api_call(api_type)\n",
    "\n",
    "            if api_data[\"status\"] == \"success\":\n",
    "                if api_type == \"production\":\n",
    "                    df = pd.json_normalize(api_data[\"data\"][\"machines\"])\n",
    "                elif api_type == \"quality\":\n",
    "                    df = pd.DataFrame([api_data[\"data\"][\"daily_summary\"]])\n",
    "                else:\n",
    "                    df = pd.DataFrame([api_data[\"data\"]])\n",
    "\n",
    "                self.processed_data[data_name] = df\n",
    "                self.log_message(f\"API Import erfolgreich: {df.shape}\")\n",
    "                return True\n",
    "            else:\n",
    "                self.log_message(\n",
    "                    f\"API Fehler: {api_data.get('message', 'Unknown error')}\", \"ERROR\"\n",
    "                )\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"API Import fehlgeschlagen: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def clean_all_data(self):\n",
    "        \"\"\"Bereinigt alle importierten Daten\"\"\"\n",
    "        if not self.config[\"clean_data\"]:\n",
    "            self.log_message(\"Datenbereinigung deaktiviert\")\n",
    "            return\n",
    "\n",
    "        self.log_message(\"Starte Datenbereinigung fÃ¼r alle Datasets\")\n",
    "\n",
    "        for data_name, df in self.processed_data.items():\n",
    "            try:\n",
    "                # Grundlegende Bereinigung\n",
    "                initial_rows = len(df)\n",
    "\n",
    "                # Duplikate entfernen\n",
    "                df_clean = df.drop_duplicates()\n",
    "\n",
    "                # Leere Spalten entfernen\n",
    "                df_clean = df_clean.dropna(axis=1, how=\"all\")\n",
    "\n",
    "                # Komplett leere Zeilen entfernen\n",
    "                df_clean = df_clean.dropna(axis=0, how=\"all\")\n",
    "\n",
    "                final_rows = len(df_clean)\n",
    "                removed = initial_rows - final_rows\n",
    "\n",
    "                self.processed_data[data_name] = df_clean\n",
    "                self.log_message(\n",
    "                    f\"{data_name}: {removed} Zeilen entfernt, {final_rows} verbleiben\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_message(\n",
    "                    f\"Bereinigung von {data_name} fehlgeschlagen: {e}\", \"ERROR\"\n",
    "                )\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Erstellt einen Zusammenfassungsbericht\"\"\"\n",
    "        self.log_message(\"Generiere Pipeline-Report\")\n",
    "\n",
    "        report = {\n",
    "            \"pipeline_info\": {\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_datasets\": len(self.processed_data),\n",
    "                \"config\": self.config,\n",
    "            },\n",
    "            \"datasets\": {},\n",
    "            \"log\": self.log,\n",
    "        }\n",
    "\n",
    "        total_rows = 0\n",
    "        total_columns = 0\n",
    "\n",
    "        for data_name, df in self.processed_data.items():\n",
    "            dataset_info = {\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": len(df.columns),\n",
    "                \"memory_mb\": df.memory_usage(deep=True).sum() / 1024**2,\n",
    "                \"missing_values\": df.isnull().sum().sum(),\n",
    "                \"dtypes\": df.dtypes.value_counts().to_dict(),\n",
    "            }\n",
    "\n",
    "            report[\"datasets\"][data_name] = dataset_info\n",
    "            total_rows += dataset_info[\"rows\"]\n",
    "            total_columns += dataset_info[\"columns\"]\n",
    "\n",
    "        report[\"pipeline_info\"][\"total_rows\"] = total_rows\n",
    "        report[\"pipeline_info\"][\"total_columns\"] = total_columns\n",
    "\n",
    "        return report\n",
    "\n",
    "    def export_all(self, include_report=True):\n",
    "        \"\"\"Exportiert alle verarbeiteten Daten\"\"\"\n",
    "        self.log_message(\"Starte Export aller Daten\")\n",
    "\n",
    "        # Output-Verzeichnis erstellen\n",
    "        output_dir = Path(self.config[\"output_dir\"])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        exported_files = []\n",
    "\n",
    "        for data_name, df in self.processed_data.items():\n",
    "            try:\n",
    "                file_path = output_dir / f\"{data_name}_processed\"\n",
    "\n",
    "                # CSV Export\n",
    "                if \"csv\" in self.config[\"export_formats\"]:\n",
    "                    csv_file = f\"{file_path}.csv\"\n",
    "                    df.to_csv(csv_file, index=False)\n",
    "                    exported_files.append(csv_file)\n",
    "\n",
    "                # Excel Export\n",
    "                if \"excel\" in self.config[\"export_formats\"]:\n",
    "                    excel_file = f\"{file_path}.xlsx\"\n",
    "                    df.to_excel(excel_file, index=False)\n",
    "                    exported_files.append(excel_file)\n",
    "\n",
    "                # JSON Export\n",
    "                if \"json\" in self.config[\"export_formats\"]:\n",
    "                    json_file = f\"{file_path}.json\"\n",
    "                    df.to_json(json_file, orient=\"records\", indent=2)\n",
    "                    exported_files.append(json_file)\n",
    "\n",
    "                self.log_message(f\"Export fÃ¼r {data_name} abgeschlossen\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_message(f\"Export von {data_name} fehlgeschlagen: {e}\", \"ERROR\")\n",
    "\n",
    "        # Pipeline-Report exportieren\n",
    "        if include_report:\n",
    "            report = self.generate_report()\n",
    "            report_file = output_dir / \"pipeline_report.json\"\n",
    "\n",
    "            with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(report, f, indent=2, default=str, ensure_ascii=False)\n",
    "\n",
    "            exported_files.append(str(report_file))\n",
    "            self.log_message(f\"Pipeline-Report: {report_file}\")\n",
    "\n",
    "        self.log_message(f\"Export abgeschlossen: {len(exported_files)} Dateien\")\n",
    "        return exported_files\n",
    "\n",
    "\n",
    "# Pipeline testen\n",
    "pipeline = BystronicDataPipeline()\n",
    "\n",
    "print(\"ğŸš€ Starte Bystronic Datenverarbeitungs-Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daten importieren\n",
    "pipeline.import_excel(excel_file, \"production\")\n",
    "pipeline.import_api(\"production\", \"live_machines\")\n",
    "pipeline.import_api(\"quality\", \"quality_metrics\")\n",
    "\n",
    "# Daten bereinigen\n",
    "pipeline.clean_all_data()\n",
    "\n",
    "# Report generieren\n",
    "report = pipeline.generate_report()\n",
    "\n",
    "print(\"\\nğŸ“Š Pipeline Zusammenfassung:\")\n",
    "print(f\"ğŸ“‚ Datasets: {report['pipeline_info']['total_datasets']}\")\n",
    "print(f\"ğŸ“ˆ Gesamt Zeilen: {report['pipeline_info']['total_rows']:,}\")\n",
    "print(f\"ğŸ”¢ Gesamt Spalten: {report['pipeline_info']['total_columns']:,}\")\n",
    "\n",
    "for dataset_name, info in report[\"datasets\"].items():\n",
    "    print(f\"  â€¢ {dataset_name}: {info['rows']:,} Zeilen Ã— {info['columns']} Spalten\")\n",
    "\n",
    "# Exportieren\n",
    "exported_files = pipeline.export_all()\n",
    "\n",
    "print(f\"\\nâœ… Pipeline abgeschlossen! {len(exported_files)} Dateien exportiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ğŸ¯ Zusammenfassung\n\n### Was Sie gelernt haben:\n\n1. **CSV-Import mit komplexen Strukturen**\n   - Intelligente Header-Erkennung\n   - Flexible Trennzeichen-UnterstÃ¼tzung\n   - Metadaten-Extraktion\n\n2. **Excel-Verarbeitung**\n   - Mehrere ArbeitsblÃ¤tter gleichzeitig laden\n   - Strukturierte Datenorganisation\n   - Format-spezifische Features nutzen\n\n3. **JSON und API-Integration**\n   - Hierarchische Datenstrukturen verarbeiten\n   - API-Simulation und echte Webservice-Aufrufe\n   - JSON-Normalisierung fÃ¼r tabellarische Daten\n\n4. **Datenbereinigung**\n   - DatenqualitÃ¤t systematisch analysieren\n   - AusreiÃŸer und Anomalien erkennen\n   - Automatische Bereinigungsregeln implementieren\n\n5. **Export-Strategien**\n   - Mehrere Formate gleichzeitig\n   - DateigrÃ¶ÃŸe und Performance berÃ¼cksichtigen\n   - Metadaten und Dokumentation einschlieÃŸen\n\n6. **Produktions-Pipeline**\n   - VollstÃ¤ndige Automatisierung\n   - Fehlerbehandlung und Logging\n   - Konfigurierbare Verarbeitungsschritte\n\n### ğŸ’¡ Tipps fÃ¼r die Praxis:\n\n- **Immer Daten validieren** bevor Verarbeitung\n- **Encoding explizit definieren** (UTF-8, Latin1, etc.)\n- **Chunk-Processing** fÃ¼r sehr groÃŸe Dateien\n- **Fehlerbehandlung** fÃ¼r robuste Pipelines\n- **Dokumentation** der Datenquellen und -transformationen\n\n### ğŸ”— NÃ¤chste Schritte:\n\n- **Modul 07**: Jupyter Notebooks fÃ¼r interaktive Analysen\n- **Modul 08**: BenutzeroberflÃ¤chen mit PyQt/PySide und Streamlit\n- **Modul 09**: VollstÃ¤ndige Praxisprojekte\n\n---\n\n*Herzlichen GlÃ¼ckwunsch! Sie beherrschen jetzt die wichtigsten Techniken fÃ¼r Datenimport und -verarbeitung in Python fÃ¼r industrielle Anwendungen.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bystronic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
