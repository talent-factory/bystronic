{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Datenimport und -export Tutorial\n",
    "\n",
    "**Python Grundkurs für Bystronic-Entwickler - Modul 06**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Lernziele\n",
    "\n",
    "In diesem Tutorial lernen Sie:\n",
    "- CSV-Dateien mit komplexen Strukturen zu importieren\n",
    "- Excel-Dateien mit mehreren Arbeitsblättern zu verarbeiten\n",
    "- JSON-Daten aus Dateien und APIs zu laden\n",
    "- Daten zu bereinigen und zu transformieren\n",
    "- Ergebnisse in verschiedenen Formaten zu exportieren\n",
    "\n",
    "## 📋 Inhalt\n",
    "1. [CSV-Import: Bystronic Maschinendaten](#csv)\n",
    "2. [Excel-Verarbeitung](#excel) \n",
    "3. [JSON und APIs](#json)\n",
    "4. [Datenbereinigung](#cleaning)\n",
    "5. [Export und Speicherung](#export)\n",
    "6. [Praxis-Pipeline](#pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Alle Bibliotheken erfolgreich importiert!\n",
      "📊 Pandas Version: 2.3.2\n",
      "🔢 NumPy Version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Importiere notwendige Bibliotheken\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Konfiguration für bessere Darstellung\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Alle Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"📊 Pandas Version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. CSV-Import: Bystronic Maschinendaten {#csv}\n",
    "\n",
    "### 🔍 Problem: Komplexe CSV-Struktur\n",
    "\n",
    "Die V084_Scope.csv Datei hat eine spezielle Struktur:\n",
    "- **Zeilen 1-6**: Metadaten (Name, Datei, Zeitstempel)\n",
    "- **Zeile 7**: Spaltennamen (nach \"Name\" Tag)\n",
    "- **Zeilen 8-21**: Weitere Metadaten (Kommentare, Datentypen, etc.)\n",
    "- **Ab Zeile 22**: Echte Messdaten\n",
    "- **Trennzeichen**: Tab (`\\t`)\n",
    "\n",
    "### 📝 Schritt 1: Datei analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyse der Datei: ../../data/large/V084_Scope.csv\n",
      "================================================================================\n",
      " 1: Name\tDistance Control\n",
      " 2: File\tO:\\Messungen\\NDC\\MP4_T001\\V084_Scope.csv\n",
      " 3: Starttime of export\t133964386997165000\tDienstag, 8. Juli 2025\t09:58:19.716\n",
      " 4: Endtime of export\t133964388292505000\tDienstag, 8. Juli 2025\t10:00:29.250\n",
      " 5: \n",
      " 6: \n",
      " 7: Name\tDISTCTRL kp\tName\tDISTCTRL d_tv\tName\tDISTCTRL a_max\tName\tDISTCTRL a_max_int\tName\tDISTCTRL v_max\t...\n",
      " 8: SymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComment\t\tSymbolComm...\n",
      " 9: Data-Type\tREAL64\tData-Type\tREAL64\tData-Type\tINT32\tData-Type\tREAL64\tData-Type\tINT32\tData-Type\tREAL64\t...\n",
      "10: SampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t1\tSampleTime[ms]\t...\n",
      "11: VariableSize\t8\tVariableSize\t8\tVariableSize\t4\tVariableSize\t8\tVariableSize\t4\tVariableSize\t8\tVariableSi...\n",
      "12: SymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBased\tFalse\tSymbolBase...\n",
      "13: IndexGroup\t1180416\tIndexGroup\t1180416\tIndexGroup\t1180416\tIndexGroup\t1180416\tIndexGroup\t1180416\tIndex...\n",
      "14: IndexOffset\t328147\tIndexOffset\t328158\tIndexOffset\t328020\tIndexOffset\t328023\tIndexOffset\t328019\tIndex...\n",
      "15: SymbolName\tDISTCTRL::kp\tSymbolName\tDISTCTRL::d_tv\tSymbolName\tDISTCTRL::a_max\tSymbolName\tDISTCTRL::a_...\n",
      "16: NetID\t192.168.100.1.1.1\tNetID\t192.168.100.1.1.1\tNetID\t192.168.100.1.1.1\tNetID\t192.168.100.1.1.1\tNetI...\n",
      "17: Port\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t551\tPort\t851\tPort\t551\tPort\t551\tP...\n",
      "18: Offset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tOffset\t0\tO...\n",
      "19: ScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tScaleFactor\t1\tSc...\n",
      "20: BitMask\t0xffffffffffffffff\tBitMask\t0xffffffffffffffff\tBitMask\t0xffffffffffffffff\tBitMask\t0xfffffffff...\n",
      "21: \n",
      "22: 0\t1\t0\t0\t0\t10000\t0\t100000000\t0\t50000\t0\t5000000\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t12\t0\t100\t0\t1\t0\t0...\n",
      "23: 1\t1\t1\t0\t1\t10000\t1\t100000000\t1\t50000\t1\t5000000\t1\t1\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t0\t1\t12\t1\t100\t1\t1\t1\t0...\n",
      "24: 2\t1\t2\t0\t2\t10000\t2\t100000000\t2\t50000\t2\t5000000\t2\t1\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t0\t2\t12\t2\t100\t2\t1\t2\t0...\n",
      "25: 3\t1\t3\t0\t3\t10000\t3\t100000000\t3\t50000\t3\t5000000\t3\t1\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t0\t3\t15\t3\t100\t3\t1\t3\t0...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pfad zur Bystronic CSV-Datei\n",
    "csv_file = \"../../data/large/V084_Scope.csv\"\n",
    "\n",
    "\n",
    "# Erste Zeilen der Datei anschauen\n",
    "def analyze_csv_structure(file_path, n_lines=25):\n",
    "    \"\"\"\n",
    "    Analysiert die Struktur einer CSV-Datei\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Analyse der Datei: {file_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n_lines:\n",
    "                break\n",
    "            # Zeige Zeilennummer und Inhalt (begrenzt)\n",
    "            content = (\n",
    "                line.strip()[:100] + \"...\" if len(line.strip()) > 100 else line.strip()\n",
    "            )\n",
    "            print(f\"{i + 1:2d}: {content}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# Analysiere die Struktur\n",
    "analyze_csv_structure(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 Schritt 2: Intelligenter CSV-Parser\n",
    "\n",
    "Wir erstellen einen Parser, der:\n",
    "- Automatisch die Header-Zeile findet\n",
    "- Den Datenbereich identifiziert\n",
    "- Flexible Trennzeichen unterstützt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bystronic_csv(\n",
    "    file_path, header_line=None, data_start_line=None, delimiter=\"\\t\", encoding=\"utf-8\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Flexibler Parser für Bystronic CSV-Dateien\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Pfad zur CSV-Datei\n",
    "    header_line : int, optional\n",
    "        Zeilennummer für Header (1-basiert, z.B. 7). Automatisch erkannt wenn None.\n",
    "    data_start_line : int, optional\n",
    "        Zeilennummer für Datenbeginn (1-basiert). Automatisch erkannt wenn None.\n",
    "    delimiter : str\n",
    "        Trennzeichen (Standard: Tab)\n",
    "    encoding : str\n",
    "        Encoding (Standard: UTF-8)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict: {\n",
    "        'metadata': dict,\n",
    "        'data': pd.DataFrame,\n",
    "        'info': dict\n",
    "    }\n",
    "    \"\"\"\n",
    "    print(f\"📂 Lade Datei: {file_path}\")\n",
    "\n",
    "    # Schritt 1: Datei einlesen\n",
    "    with open(file_path, encoding=encoding, errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Schritt 2: Metadaten extrahieren (erste 6 Zeilen)\n",
    "    metadata = {}\n",
    "    for i in range(min(6, len(lines))):\n",
    "        parts = lines[i].strip().split(delimiter, 1)\n",
    "        if len(parts) == 2:\n",
    "            key, value = parts\n",
    "            metadata[key] = value\n",
    "\n",
    "    # Schritt 3: Header-Zeile bestimmen\n",
    "    columns = []\n",
    "    if header_line is not None:\n",
    "        # Benutzer hat Header-Zeile explizit angegeben\n",
    "        header_row_index = header_line - 1  # Konvertiere zu 0-basiert\n",
    "        if header_row_index < len(lines):\n",
    "            header_content = lines[header_row_index].strip()\n",
    "            print(f\"📋 Verwende Header-Zeile {header_line}: {header_content[:100]}...\")\n",
    "\n",
    "            parts = header_content.split(delimiter)\n",
    "            # Extrahiere Spaltennamen (jede zweite Spalte nach \"Name\")\n",
    "            for j in range(1, len(parts), 2):\n",
    "                if j < len(parts) and parts[j].strip():\n",
    "                    columns.append(parts[j].strip())\n",
    "        else:\n",
    "            print(\n",
    "                f\"⚠️ Header-Zeile {header_line} existiert nicht (nur {len(lines)} Zeilen)\"\n",
    "            )\n",
    "    else:\n",
    "        # Automatische Header-Erkennung\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(\"Name\" + delimiter):\n",
    "                header_row_index = i\n",
    "                header_line = i + 1\n",
    "                parts = line.strip().split(delimiter)\n",
    "                # Extrahiere Spaltennamen\n",
    "                for j in range(1, len(parts), 2):\n",
    "                    if j < len(parts) and parts[j].strip():\n",
    "                        columns.append(parts[j].strip())\n",
    "                print(f\"📋 Header automatisch erkannt in Zeile {header_line}\")\n",
    "                break\n",
    "\n",
    "    # Schritt 4: Datenbereich bestimmen\n",
    "    if data_start_line is not None:\n",
    "        # Benutzer hat Datenstart explizit angegeben\n",
    "        data_start_row_index = data_start_line - 1\n",
    "        print(f\"📊 Verwende Datenstart-Zeile {data_start_line}\")\n",
    "    else:\n",
    "        # Automatische Datenbereich-Erkennung\n",
    "        data_start_row_index = None\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i].strip()\n",
    "            if line and not any(\n",
    "                keyword in line\n",
    "                for keyword in [\n",
    "                    \"Name\",\n",
    "                    \"Data-Type\",\n",
    "                    \"SampleTime\",\n",
    "                    \"SymbolComment\",\n",
    "                    \"VariableSize\",\n",
    "                    \"SymbolBased\",\n",
    "                    \"IndexGroup\",\n",
    "                ]\n",
    "            ):\n",
    "                # Prüfe ob Zeile hauptsächlich aus Zahlen besteht\n",
    "                parts = line.split(delimiter)\n",
    "                numeric_count = 0\n",
    "                for part in parts[: min(10, len(parts))]:  # Prüfe erste 10 Spalten\n",
    "                    try:\n",
    "                        float(part)\n",
    "                        numeric_count += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                if (\n",
    "                    numeric_count >= min(len(parts), 10) * 0.7\n",
    "                ):  # 70% der Werte sind Zahlen\n",
    "                    data_start_row_index = i\n",
    "                    data_start_line = i + 1\n",
    "                    print(\n",
    "                        f\"📊 Datenbereich automatisch erkannt ab Zeile {data_start_line}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "    print(f\"🔢 Anzahl Spalten gefunden: {len(columns)}\")\n",
    "    if columns:\n",
    "        print(\n",
    "            f\"📝 Beispiel-Spalten: {', '.join(columns[:5])}{'...' if len(columns) > 5 else ''}\"\n",
    "        )\n",
    "\n",
    "    # Schritt 5: DataFrame erstellen\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    if data_start_row_index is not None and columns:\n",
    "        try:\n",
    "            # Datenzeilen extrahieren\n",
    "            data_lines = lines[data_start_row_index:]\n",
    "\n",
    "            parsed_data = []\n",
    "            for line_num, line in enumerate(data_lines):\n",
    "                parts = line.strip().split(delimiter)\n",
    "                if len(parts) >= len(columns):\n",
    "                    parsed_data.append(parts[: len(columns)])\n",
    "\n",
    "                # Begrenzte Anzahl für Demo (kann entfernt werden für vollständigen Import)\n",
    "                if len(parsed_data) >= 1000:  # Lade max. 1000 Zeilen für Demo\n",
    "                    print(\"ℹ️ Demo-Modus: Nur erste 1000 Zeilen geladen\")\n",
    "                    break\n",
    "\n",
    "            if parsed_data:\n",
    "                df = pd.DataFrame(parsed_data, columns=columns)\n",
    "\n",
    "                # Datentyp-Konvertierung\n",
    "                print(\"🔄 Konvertiere Datentypen...\")\n",
    "                for col in df.columns:\n",
    "                    try:\n",
    "                        # Versuche numerische Konvertierung\n",
    "                        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                    except:\n",
    "                        # Behalte als String bei Fehlern\n",
    "                        pass\n",
    "\n",
    "                print(\n",
    "                    f\"✅ DataFrame erstellt: {df.shape[0]:,} Zeilen × {df.shape[1]} Spalten\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"⚠️ Keine gültigen Datenzeilen gefunden\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Fehler beim DataFrame-Erstellen: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"metadata\": metadata,\n",
    "        \"data\": df,\n",
    "        \"info\": {\n",
    "            \"header_line\": header_line,\n",
    "            \"data_start_line\": data_start_line,\n",
    "            \"total_lines\": len(lines),\n",
    "            \"columns\": columns,\n",
    "            \"parsing_success\": not df.empty,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Parser testen - Benutzer kann Header-Zeile explizit angeben\n",
    "print(\"🔧 Test 1: Automatische Erkennung\")\n",
    "result_auto = parse_bystronic_csv(csv_file)\n",
    "\n",
    "print(\"\\n🔧 Test 2: Header-Zeile 7 explizit angeben\")\n",
    "result_manual = parse_bystronic_csv(csv_file, header_line=7, data_start_line=22)\n",
    "\n",
    "print(\"\\n📋 Metadaten:\")\n",
    "for key, value in result_manual[\"metadata\"].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n📊 DataFrame Info:\")\n",
    "if not result_manual[\"data\"].empty:\n",
    "    print(result_manual[\"data\"].info())\n",
    "    print(\"\\n🔍 Erste 5 Zeilen:\")\n",
    "    print(result_manual[\"data\"].head())\n",
    "else:\n",
    "    print(\"❌ Kein DataFrame erstellt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 Schritt 3: Daten erkunden und visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame für einfachere Verwendung (verwende die manuell geparste Version)\n",
    "df = result_manual[\"data\"]\n",
    "\n",
    "print(\"🔍 Erste 5 Zeilen:\")\n",
    "if not df.empty:\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\n📈 Statistische Übersicht:\")\n",
    "    display(df.describe())\n",
    "\n",
    "    # Interessante Spalten für Visualisierung finden\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\n🔢 Numerische Spalten ({len(numeric_cols)}):\")\n",
    "    for i, col in enumerate(numeric_cols[:10]):  # Erste 10 anzeigen\n",
    "        print(f\"  {i + 1:2d}. {col}\")\n",
    "\n",
    "    if len(numeric_cols) >= 2:\n",
    "        # Beispiel-Visualisierung\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\"🔧 Bystronic Maschinendaten - Analyse\", fontsize=16)\n",
    "\n",
    "        # Plot 1: Zeitreihe der ersten numerischen Spalte\n",
    "        col1 = numeric_cols[0]\n",
    "        axes[0, 0].plot(df.index, df[col1], alpha=0.7)\n",
    "        axes[0, 0].set_title(f\"Zeitverlauf: {col1}\")\n",
    "        axes[0, 0].set_xlabel(\"Messung\")\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 2: Histogram\n",
    "        col2 = numeric_cols[1] if len(numeric_cols) > 1 else numeric_cols[0]\n",
    "        axes[0, 1].hist(df[col2].dropna(), bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "        axes[0, 1].set_title(f\"Verteilung: {col2}\")\n",
    "        axes[0, 1].set_xlabel(\"Wert\")\n",
    "        axes[0, 1].set_ylabel(\"Häufigkeit\")\n",
    "\n",
    "        # Plot 3: Scatter Plot (falls genügend Spalten)\n",
    "        if len(numeric_cols) >= 3:\n",
    "            col3 = numeric_cols[2]\n",
    "            axes[1, 0].scatter(df[col1], df[col3], alpha=0.6, s=20)\n",
    "            axes[1, 0].set_title(f\"Korrelation: {col1} vs {col3}\")\n",
    "            axes[1, 0].set_xlabel(col1)\n",
    "            axes[1, 0].set_ylabel(col3)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Boxplot der ersten 5 numerischen Spalten\n",
    "        sample_cols = numeric_cols[:5]\n",
    "        df_sample = df[sample_cols]\n",
    "        # Normalisiere Daten für bessere Darstellung\n",
    "        df_normalized = (df_sample - df_sample.mean()) / df_sample.std()\n",
    "        axes[1, 1].boxplot(\n",
    "            [df_normalized[col].dropna() for col in sample_cols],\n",
    "            labels=[col[:15] + \"...\" if len(col) > 15 else col for col in sample_cols],\n",
    "        )\n",
    "        axes[1, 1].set_title(\"Normalisierte Boxplots\")\n",
    "        axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"✅ Visualisierung mit {len(sample_cols)} Parametern erstellt\")\n",
    "    else:\n",
    "        print(\"ℹ️ Nicht genügend numerische Spalten für erweiterte Visualisierung\")\n",
    "else:\n",
    "    print(\"❌ Kein DataFrame verfügbar für Analyse\")\n",
    "\n",
    "# Zeige Verwendungsbeispiele\n",
    "print(\"\\n💡 Verwendungsbeispiele:\")\n",
    "print(\"# Automatische Erkennung:\")\n",
    "print(\"result = parse_bystronic_csv('V084_Scope.csv')\")\n",
    "print(\"\")\n",
    "print(\"# Header-Zeile 7, Daten ab Zeile 22:\")\n",
    "print(\n",
    "    \"result = parse_bystronic_csv('V084_Scope.csv', header_line=7, data_start_line=22)\"\n",
    ")\n",
    "print(\"\")\n",
    "print(\"# Nur Header-Zeile angeben:\")\n",
    "print(\"result = parse_bystronic_csv('V084_Scope.csv', header_line=7)\")\n",
    "print(\"\")\n",
    "print(\"# DataFrame zugreifen:\")\n",
    "print(\"df = result['data']\")\n",
    "print(\"metadata = result['metadata']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Excel-Verarbeitung {#excel}\n",
    "\n",
    "### 📝 Erstelle Beispiel-Excel-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine Beispiel-Excel-Datei mit mehreren Arbeitsblättern\n",
    "def create_sample_excel(file_path):\n",
    "    \"\"\"\n",
    "    Erstellt eine Beispiel-Excel-Datei für Bystronic\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "        # Arbeitsblatt 1: Produktionsdaten\n",
    "        np.random.seed(42)\n",
    "        dates = pd.date_range(\"2024-01-01\", \"2024-12-31\", freq=\"D\")\n",
    "        production_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Datum\": dates,\n",
    "                \"Maschine_A_Stück\": np.random.normal(1000, 100, len(dates)).astype(int),\n",
    "                \"Maschine_B_Stück\": np.random.normal(800, 80, len(dates)).astype(int),\n",
    "                \"Maschine_C_Stück\": np.random.normal(1200, 150, len(dates)).astype(int),\n",
    "                \"Verfügbarkeit_A_%\": np.random.normal(85, 5, len(dates)),\n",
    "                \"Verfügbarkeit_B_%\": np.random.normal(82, 6, len(dates)),\n",
    "                \"Verfügbarkeit_C_%\": np.random.normal(88, 4, len(dates)),\n",
    "            }\n",
    "        )\n",
    "        production_data.to_excel(writer, sheet_name=\"Produktion\", index=False)\n",
    "\n",
    "        # Arbeitsblatt 2: Qualitätsdaten\n",
    "        quality_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Monat\": pd.date_range(\"2024-01-01\", \"2024-12-01\", freq=\"M\"),\n",
    "                \"Ausschuss_Rate_%\": np.random.normal(2.5, 0.5, 12),\n",
    "                \"Nacharbeit_Rate_%\": np.random.normal(1.2, 0.3, 12),\n",
    "                \"Kundenzufriedenheit\": np.random.normal(4.2, 0.2, 12),\n",
    "                \"Reklamationen_Anzahl\": np.random.poisson(5, 12),\n",
    "            }\n",
    "        )\n",
    "        quality_data.to_excel(writer, sheet_name=\"Qualität\", index=False)\n",
    "\n",
    "        # Arbeitsblatt 3: Wartungsdaten\n",
    "        maintenance_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Maschine\": [\"A\", \"B\", \"C\"] * 50,\n",
    "                \"Wartungsdatum\": pd.date_range(\"2024-01-01\", periods=150, freq=\"3D\"),\n",
    "                \"Typ\": np.random.choice([\"Präventiv\", \"Korrektiv\", \"Inspektion\"], 150),\n",
    "                \"Dauer_h\": np.random.exponential(2, 150),\n",
    "                \"Kosten_EUR\": np.random.gamma(2, 500, 150),\n",
    "            }\n",
    "        )\n",
    "        maintenance_data.to_excel(writer, sheet_name=\"Wartung\", index=False)\n",
    "\n",
    "        # Arbeitsblatt 4: Konfiguration\n",
    "        config_data = pd.DataFrame(\n",
    "            {\n",
    "                \"Parameter\": [\n",
    "                    \"Max_Geschwindigkeit\",\n",
    "                    \"Toleranz_X\",\n",
    "                    \"Toleranz_Y\",\n",
    "                    \"Laser_Power\",\n",
    "                    \"Kühlmittel_Temp\",\n",
    "                ],\n",
    "                \"Maschine_A\": [2500, 0.05, 0.05, 6000, 20],\n",
    "                \"Maschine_B\": [2200, 0.08, 0.06, 5500, 22],\n",
    "                \"Maschine_C\": [2800, 0.04, 0.04, 6500, 18],\n",
    "                \"Einheit\": [\"mm/min\", \"mm\", \"mm\", \"W\", \"°C\"],\n",
    "            }\n",
    "        )\n",
    "        config_data.to_excel(writer, sheet_name=\"Konfiguration\", index=False)\n",
    "\n",
    "    print(f\"✅ Excel-Datei erstellt: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# Excel-Datei erstellen\n",
    "excel_file = \"../../data/bystronic_production_data.xlsx\"\n",
    "create_sample_excel(excel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 Excel-Datei laden und analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel-Datei mit allen Arbeitsblättern laden\n",
    "def load_excel_comprehensive(file_path):\n",
    "    \"\"\"\n",
    "    Lädt eine Excel-Datei mit allen Arbeitsblättern\n",
    "    \"\"\"\n",
    "    print(f\"📂 Lade Excel-Datei: {file_path}\")\n",
    "\n",
    "    # Alle Arbeitsblätter laden\n",
    "    excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    print(f\"📋 Arbeitsblätter gefunden: {list(excel_data.keys())}\")\n",
    "\n",
    "    for sheet_name, df in excel_data.items():\n",
    "        print(f\"\\n📊 {sheet_name}: {df.shape[0]} Zeilen × {df.shape[1]} Spalten\")\n",
    "        print(\n",
    "            f\"   Spalten: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\"\n",
    "        )\n",
    "\n",
    "    return excel_data\n",
    "\n",
    "\n",
    "# Excel laden\n",
    "excel_data = load_excel_comprehensive(excel_file)\n",
    "\n",
    "# Einzelne Arbeitsblätter analysieren\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📋 PRODUKTIONSDATEN ANALYSE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "production_df = excel_data[\"Produktion\"]\n",
    "display(production_df.head())\n",
    "\n",
    "print(\"\\n📈 Produktionsstatistiken:\")\n",
    "production_stats = production_df.select_dtypes(include=[np.number]).describe()\n",
    "display(production_stats)\n",
    "\n",
    "# Visualisierung der Produktionsdaten\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"🏭 Bystronic Produktionsanalyse\", fontsize=16)\n",
    "\n",
    "# Plot 1: Produktionsmenge über Zeit\n",
    "production_df.plot(\n",
    "    x=\"Datum\",\n",
    "    y=[\"Maschine_A_Stück\", \"Maschine_B_Stück\", \"Maschine_C_Stück\"],\n",
    "    ax=axes[0, 0],\n",
    "    title=\"Produktionsmenge pro Tag\",\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Datum\")\n",
    "axes[0, 0].set_ylabel(\"Stück pro Tag\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Verfügbarkeit über Zeit\n",
    "production_df.plot(\n",
    "    x=\"Datum\",\n",
    "    y=[\"Verfügbarkeit_A_%\", \"Verfügbarkeit_B_%\", \"Verfügbarkeit_C_%\"],\n",
    "    ax=axes[0, 1],\n",
    "    title=\"Maschinenverfügbarkeit\",\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Datum\")\n",
    "axes[0, 1].set_ylabel(\"Verfügbarkeit %\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Korrelation Produktion vs Verfügbarkeit\n",
    "axes[1, 0].scatter(\n",
    "    production_df[\"Verfügbarkeit_A_%\"], production_df[\"Maschine_A_Stück\"], alpha=0.6\n",
    ")\n",
    "axes[1, 0].set_title(\"Korrelation: Verfügbarkeit vs Produktion (Maschine A)\")\n",
    "axes[1, 0].set_xlabel(\"Verfügbarkeit %\")\n",
    "axes[1, 0].set_ylabel(\"Produktion Stück/Tag\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Monatliche Zusammenfassung\n",
    "monthly_prod = production_df.copy()\n",
    "monthly_prod[\"Monat\"] = monthly_prod[\"Datum\"].dt.to_period(\"M\")\n",
    "monthly_summary = monthly_prod.groupby(\"Monat\")[\n",
    "    [\"Maschine_A_Stück\", \"Maschine_B_Stück\", \"Maschine_C_Stück\"]\n",
    "].sum()\n",
    "monthly_summary.plot(kind=\"bar\", ax=axes[1, 1], title=\"Monatsproduktion\")\n",
    "axes[1, 1].set_xlabel(\"Monat\")\n",
    "axes[1, 1].set_ylabel(\"Gesamt Stück\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. JSON und APIs {#json}\n",
    "\n",
    "### 📝 JSON-Daten verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel JSON für IoT-Sensordaten erstellen\n",
    "def create_sample_json(file_path):\n",
    "    \"\"\"\n",
    "    Erstellt eine Beispiel-JSON-Datei mit IoT-Sensordaten\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    sensor_data = {\n",
    "        \"metadata\": {\n",
    "            \"system\": \"Bystronic IoT Platform\",\n",
    "            \"version\": \"2.1.0\",\n",
    "            \"timestamp\": \"2024-01-15T08:30:00Z\",\n",
    "            \"location\": \"Werk_Niederönz\",\n",
    "        },\n",
    "        \"sensors\": [\n",
    "            {\n",
    "                \"id\": \"TEMP_001\",\n",
    "                \"type\": \"temperature\",\n",
    "                \"location\": \"Laser_Cutting_1\",\n",
    "                \"unit\": \"celsius\",\n",
    "                \"readings\": [\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:30:00Z\",\n",
    "                        \"value\": 22.5,\n",
    "                        \"status\": \"ok\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:31:00Z\",\n",
    "                        \"value\": 22.8,\n",
    "                        \"status\": \"ok\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:32:00Z\",\n",
    "                        \"value\": 23.1,\n",
    "                        \"status\": \"ok\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:33:00Z\",\n",
    "                        \"value\": 23.4,\n",
    "                        \"status\": \"warning\",\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"VIBR_001\",\n",
    "                \"type\": \"vibration\",\n",
    "                \"location\": \"Laser_Cutting_1\",\n",
    "                \"unit\": \"mm/s\",\n",
    "                \"readings\": [\n",
    "                    {\"timestamp\": \"2024-01-15T08:30:00Z\", \"value\": 1.2, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:31:00Z\", \"value\": 1.4, \"status\": \"ok\"},\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:32:00Z\",\n",
    "                        \"value\": 2.1,\n",
    "                        \"status\": \"warning\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"timestamp\": \"2024-01-15T08:33:00Z\",\n",
    "                        \"value\": 2.8,\n",
    "                        \"status\": \"error\",\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"POWER_001\",\n",
    "                \"type\": \"power\",\n",
    "                \"location\": \"Laser_Cutting_1\",\n",
    "                \"unit\": \"kW\",\n",
    "                \"readings\": [\n",
    "                    {\"timestamp\": \"2024-01-15T08:30:00Z\", \"value\": 6.2, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:31:00Z\", \"value\": 6.0, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:32:00Z\", \"value\": 5.8, \"status\": \"ok\"},\n",
    "                    {\"timestamp\": \"2024-01-15T08:33:00Z\", \"value\": 5.5, \"status\": \"ok\"},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        \"alerts\": [\n",
    "            {\n",
    "                \"id\": \"ALT_001\",\n",
    "                \"timestamp\": \"2024-01-15T08:33:00Z\",\n",
    "                \"severity\": \"high\",\n",
    "                \"sensor_id\": \"VIBR_001\",\n",
    "                \"message\": \"Vibration level exceeds threshold\",\n",
    "                \"threshold\": 2.5,\n",
    "                \"actual_value\": 2.8,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sensor_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ JSON-Datei erstellt: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# JSON-Datei erstellen\n",
    "json_file = \"../../data/sensor_data.json\"\n",
    "create_sample_json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON-Daten laden und verarbeiten\n",
    "def process_sensor_json(file_path):\n",
    "    \"\"\"\n",
    "    Verarbeitet JSON-Sensordaten und erstellt DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"📂 Lade JSON-Datei: {file_path}\")\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"🏭 System: {data['metadata']['system']}\")\n",
    "    print(f\"📍 Standort: {data['metadata']['location']}\")\n",
    "    print(f\"📊 Sensoren: {len(data['sensors'])}\")\n",
    "    print(f\"⚠️ Alerts: {len(data['alerts'])}\")\n",
    "\n",
    "    # Sensordaten in DataFrame umwandeln\n",
    "    sensor_readings = []\n",
    "\n",
    "    for sensor in data[\"sensors\"]:\n",
    "        sensor_id = sensor[\"id\"]\n",
    "        sensor_type = sensor[\"type\"]\n",
    "        location = sensor[\"location\"]\n",
    "        unit = sensor[\"unit\"]\n",
    "\n",
    "        for reading in sensor[\"readings\"]:\n",
    "            sensor_readings.append(\n",
    "                {\n",
    "                    \"sensor_id\": sensor_id,\n",
    "                    \"type\": sensor_type,\n",
    "                    \"location\": location,\n",
    "                    \"unit\": unit,\n",
    "                    \"timestamp\": pd.to_datetime(reading[\"timestamp\"]),\n",
    "                    \"value\": reading[\"value\"],\n",
    "                    \"status\": reading[\"status\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_sensors = pd.DataFrame(sensor_readings)\n",
    "\n",
    "    # Alerts DataFrame\n",
    "    df_alerts = pd.DataFrame(data[\"alerts\"])\n",
    "    if not df_alerts.empty:\n",
    "        df_alerts[\"timestamp\"] = pd.to_datetime(df_alerts[\"timestamp\"])\n",
    "\n",
    "    return df_sensors, df_alerts, data[\"metadata\"]\n",
    "\n",
    "\n",
    "# JSON verarbeiten\n",
    "df_sensors, df_alerts, metadata = process_sensor_json(json_file)\n",
    "\n",
    "print(\"\\n📊 Sensordaten:\")\n",
    "display(df_sensors)\n",
    "\n",
    "print(\"\\n⚠️ Alerts:\")\n",
    "display(df_alerts)\n",
    "\n",
    "# Visualisierung der Sensordaten\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"🔧 IoT Sensordaten Analysis\", fontsize=16)\n",
    "\n",
    "# Plot für jeden Sensortyp\n",
    "sensor_types = df_sensors[\"type\"].unique()\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\"]\n",
    "\n",
    "for i, sensor_type in enumerate(sensor_types[:4]):\n",
    "    row, col = i // 2, i % 2\n",
    "    sensor_data = df_sensors[df_sensors[\"type\"] == sensor_type]\n",
    "\n",
    "    axes[row, col].plot(\n",
    "        sensor_data[\"timestamp\"],\n",
    "        sensor_data[\"value\"],\n",
    "        color=colors[i],\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        markersize=6,\n",
    "    )\n",
    "    axes[row, col].set_title(f\"{sensor_type.upper()} Verlauf\")\n",
    "    axes[row, col].set_xlabel(\"Zeit\")\n",
    "    axes[row, col].set_ylabel(f\"{sensor_data.iloc[0]['unit']}\")\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "    # Status-basierte Farbcodierung\n",
    "    for status in [\"ok\", \"warning\", \"error\"]:\n",
    "        status_data = sensor_data[sensor_data[\"status\"] == status]\n",
    "        if not status_data.empty:\n",
    "            color_map = {\"ok\": \"green\", \"warning\": \"orange\", \"error\": \"red\"}\n",
    "            axes[row, col].scatter(\n",
    "                status_data[\"timestamp\"],\n",
    "                status_data[\"value\"],\n",
    "                c=color_map[status],\n",
    "                s=100,\n",
    "                alpha=0.7,\n",
    "                label=status,\n",
    "                edgecolors=\"black\",\n",
    "                linewidth=1,\n",
    "            )\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 API-Daten simulieren (Web Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuliere API-Aufruf (da wir keine echte API haben)\n",
    "def simulate_api_call(api_type=\"production\"):\n",
    "    \"\"\"\n",
    "    Simuliert verschiedene API-Aufrufe für Bystronic-Systeme\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    print(f\"🌐 Simuliere API-Aufruf: {api_type}\")\n",
    "    time.sleep(1)  # Simuliere Netzwerk-Latenz\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if api_type == \"production\":\n",
    "        # Produktions-API\n",
    "        data = {\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"data\": {\n",
    "                \"machines\": [\n",
    "                    {\n",
    "                        \"id\": \"LC001\",\n",
    "                        \"name\": \"Laser_Cutting_1\",\n",
    "                        \"status\": \"running\",\n",
    "                        \"current_job\": \"JOB_2024_001\",\n",
    "                        \"parts_produced_today\": np.random.randint(800, 1200),\n",
    "                        \"efficiency\": np.random.uniform(85, 95),\n",
    "                        \"temperature\": np.random.uniform(20, 25),\n",
    "                    },\n",
    "                    {\n",
    "                        \"id\": \"LC002\",\n",
    "                        \"name\": \"Laser_Cutting_2\",\n",
    "                        \"status\": \"maintenance\",\n",
    "                        \"current_job\": None,\n",
    "                        \"parts_produced_today\": 0,\n",
    "                        \"efficiency\": 0,\n",
    "                        \"temperature\": np.random.uniform(18, 22),\n",
    "                    },\n",
    "                    {\n",
    "                        \"id\": \"BR001\",\n",
    "                        \"name\": \"Press_Brake_1\",\n",
    "                        \"status\": \"running\",\n",
    "                        \"current_job\": \"JOB_2024_002\",\n",
    "                        \"parts_produced_today\": np.random.randint(600, 900),\n",
    "                        \"efficiency\": np.random.uniform(80, 90),\n",
    "                        \"temperature\": np.random.uniform(22, 28),\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "\n",
    "    elif api_type == \"quality\":\n",
    "        # Qualitäts-API\n",
    "        data = {\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"data\": {\n",
    "                \"daily_summary\": {\n",
    "                    \"total_parts\": np.random.randint(2000, 3000),\n",
    "                    \"defective_parts\": np.random.randint(10, 50),\n",
    "                    \"defect_rate\": np.random.uniform(0.5, 2.5),\n",
    "                    \"rework_rate\": np.random.uniform(0.2, 1.0),\n",
    "                },\n",
    "                \"defect_types\": [\n",
    "                    {\"type\": \"dimensional\", \"count\": np.random.randint(5, 20)},\n",
    "                    {\"type\": \"surface\", \"count\": np.random.randint(2, 15)},\n",
    "                    {\"type\": \"material\", \"count\": np.random.randint(1, 8)},\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # Allgemeine API\n",
    "        data = {\"status\": \"error\", \"message\": \"Unknown API endpoint\"}\n",
    "\n",
    "    print(f\"✅ API Response: {data['status']}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# API-Aufrufe testen\n",
    "production_data = simulate_api_call(\"production\")\n",
    "quality_data = simulate_api_call(\"quality\")\n",
    "\n",
    "print(\"\\n📊 Produktions-API Daten:\")\n",
    "production_df = pd.json_normalize(production_data[\"data\"][\"machines\"])\n",
    "display(production_df)\n",
    "\n",
    "print(\"\\n🔍 Qualitäts-API Daten:\")\n",
    "quality_summary = quality_data[\"data\"][\"daily_summary\"]\n",
    "defects_df = pd.DataFrame(quality_data[\"data\"][\"defect_types\"])\n",
    "\n",
    "print(f\"Tagesproduktion: {quality_summary['total_parts']} Teile\")\n",
    "print(f\"Ausschussrate: {quality_summary['defect_rate']:.2f}%\")\n",
    "print(\"\\nFehlertypen:\")\n",
    "display(defects_df)\n",
    "\n",
    "# Visualisierung der API-Daten\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"🔄 Live API-Daten Dashboard\", fontsize=16)\n",
    "\n",
    "# Plot 1: Maschineneffizienz\n",
    "running_machines = production_df[production_df[\"status\"] == \"running\"]\n",
    "if not running_machines.empty:\n",
    "    bars1 = axes[0].bar(\n",
    "        running_machines[\"name\"],\n",
    "        running_machines[\"efficiency\"],\n",
    "        color=[\n",
    "            \"green\" if x > 90 else \"orange\" if x > 80 else \"red\"\n",
    "            for x in running_machines[\"efficiency\"]\n",
    "        ],\n",
    "    )\n",
    "    axes[0].set_title(\"Maschineneffizienz (%)\")\n",
    "    axes[0].set_ylabel(\"Effizienz %\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Tagesproduktion\n",
    "all_machines = production_df[production_df[\"parts_produced_today\"] > 0]\n",
    "if not all_machines.empty:\n",
    "    bars2 = axes[1].bar(\n",
    "        all_machines[\"name\"],\n",
    "        all_machines[\"parts_produced_today\"],\n",
    "        color=\"steelblue\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[1].set_title(\"Tagesproduktion (Teile)\")\n",
    "    axes[1].set_ylabel(\"Anzahl Teile\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Fehlertypen\n",
    "if not defects_df.empty:\n",
    "    colors = [\"lightcoral\", \"lightsalmon\", \"lightblue\"]\n",
    "    bars3 = axes[2].pie(\n",
    "        defects_df[\"count\"],\n",
    "        labels=defects_df[\"type\"],\n",
    "        colors=colors,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "    )\n",
    "    axes[2].set_title(\"Fehlerverteilung\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Datenbereinigung {#cleaning}\n",
    "\n",
    "### 📝 Datenqualität prüfen und verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen für Datenbereinigung\n",
    "def analyze_data_quality(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Analysiert die Datenqualität eines DataFrames\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Datenqualitätsanalyse: {name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"📊 Shape: {df.shape[0]} Zeilen × {df.shape[1]} Spalten\")\n",
    "    print(f\"💾 Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Fehlende Werte\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\n❌ Fehlende Werte:\")\n",
    "        missing_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"Spalte\": missing_data.index,\n",
    "                \"Fehlend\": missing_data.values,\n",
    "                \"Prozent\": missing_percent.values,\n",
    "            }\n",
    "        )\n",
    "        missing_summary = missing_summary[missing_summary[\"Fehlend\"] > 0].sort_values(\n",
    "            \"Fehlend\", ascending=False\n",
    "        )\n",
    "        display(missing_summary)\n",
    "    else:\n",
    "        print(\"\\n✅ Keine fehlenden Werte gefunden\")\n",
    "\n",
    "    # Datentypen\n",
    "    print(\"\\n📈 Datentypen:\")\n",
    "    dtype_summary = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_summary.items():\n",
    "        print(f\"  {dtype}: {count} Spalten\")\n",
    "\n",
    "    # Duplikate\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\n⚠️ Duplikate: {duplicates} Zeilen ({duplicates / len(df) * 100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n✅ Keine Duplikate gefunden\")\n",
    "\n",
    "    # Numerische Spalten analysieren\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\n🔢 Numerische Analyse ({len(numeric_cols)} Spalten):\")\n",
    "        for col in numeric_cols[:5]:  # Erste 5 Spalten\n",
    "            series = df[col].dropna()\n",
    "            if len(series) > 0:\n",
    "                outliers = detect_outliers_iqr(series)\n",
    "                print(\n",
    "                    f\"  {col}: Min={series.min():.2f}, Max={series.max():.2f}, Outliers={len(outliers)}\"\n",
    "                )\n",
    "\n",
    "    return {\n",
    "        \"shape\": df.shape,\n",
    "        \"missing_values\": missing_data.sum(),\n",
    "        \"duplicates\": duplicates,\n",
    "        \"memory_mb\": df.memory_usage(deep=True).sum() / 1024**2,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(series, k=1.5):\n",
    "    \"\"\"\n",
    "    Erkennt Ausreißer mit der IQR-Methode\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "# Analysiere verschiedene Datasets\n",
    "if not result[\"data\"].empty:\n",
    "    bystronic_quality = analyze_data_quality(result[\"data\"], \"Bystronic CSV\")\n",
    "\n",
    "production_quality = analyze_data_quality(production_df, \"Produktionsdaten\")\n",
    "sensors_quality = analyze_data_quality(df_sensors, \"Sensordaten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenbereinigung durchführen\n",
    "def clean_production_data(df):\n",
    "    \"\"\"\n",
    "    Bereinigt Produktionsdaten\n",
    "    \"\"\"\n",
    "    print(\"🧹 Starte Datenbereinigung...\")\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Duplikate entfernen\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    removed_duplicates = initial_rows - len(df_clean)\n",
    "    if removed_duplicates > 0:\n",
    "        print(f\"✅ {removed_duplicates} Duplikate entfernt\")\n",
    "\n",
    "    # 2. Unrealistische Werte korrigieren\n",
    "    # Verfügbarkeit sollte zwischen 0-100% liegen\n",
    "    availability_cols = [col for col in df_clean.columns if \"Verfügbarkeit\" in col]\n",
    "    for col in availability_cols:\n",
    "        # Werte > 100% auf 100% setzen\n",
    "        outliers_high = df_clean[col] > 100\n",
    "        if outliers_high.sum() > 0:\n",
    "            df_clean.loc[outliers_high, col] = 100\n",
    "            print(f\"✅ {outliers_high.sum()} hohe Ausreißer in {col} korrigiert\")\n",
    "\n",
    "        # Negative Werte auf 0 setzen\n",
    "        negative = df_clean[col] < 0\n",
    "        if negative.sum() > 0:\n",
    "            df_clean.loc[negative, col] = 0\n",
    "            print(f\"✅ {negative.sum()} negative Werte in {col} korrigiert\")\n",
    "\n",
    "    # 3. Produktionswerte können nicht negativ sein\n",
    "    production_cols = [col for col in df_clean.columns if \"Stück\" in col]\n",
    "    for col in production_cols:\n",
    "        negative = df_clean[col] < 0\n",
    "        if negative.sum() > 0:\n",
    "            df_clean.loc[negative, col] = 0\n",
    "            print(f\"✅ {negative.sum()} negative Produktionswerte in {col} korrigiert\")\n",
    "\n",
    "        # Extreme Ausreißer (> 99.9% Quantil) glätten\n",
    "        threshold = df_clean[col].quantile(0.999)\n",
    "        extreme_outliers = df_clean[col] > threshold\n",
    "        if extreme_outliers.sum() > 0:\n",
    "            df_clean.loc[extreme_outliers, col] = threshold\n",
    "            print(f\"✅ {extreme_outliers.sum()} extreme Ausreißer in {col} geglättet\")\n",
    "\n",
    "    # 4. Datum validieren\n",
    "    if \"Datum\" in df_clean.columns:\n",
    "        # Sicherstellen, dass Datum im erwarteten Bereich liegt\n",
    "        min_date = pd.Timestamp(\"2020-01-01\")\n",
    "        max_date = pd.Timestamp(\"2030-12-31\")\n",
    "\n",
    "        invalid_dates = (df_clean[\"Datum\"] < min_date) | (df_clean[\"Datum\"] > max_date)\n",
    "        if invalid_dates.sum() > 0:\n",
    "            print(f\"⚠️ {invalid_dates.sum()} ungültige Datumswerte gefunden\")\n",
    "            df_clean = df_clean[~invalid_dates]\n",
    "\n",
    "    print(f\"🎉 Bereinigung abgeschlossen: {len(df_clean)} Zeilen verbleiben\")\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Produktionsdaten bereinigen\n",
    "production_clean = clean_production_data(production_df)\n",
    "\n",
    "# Vorher/Nachher Vergleich\n",
    "print(\"\\n📊 Bereinigung - Vorher/Nachher:\")\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Vorher\": [\n",
    "            len(production_df),\n",
    "            production_df.isnull().sum().sum(),\n",
    "            production_df.duplicated().sum(),\n",
    "        ],\n",
    "        \"Nachher\": [\n",
    "            len(production_clean),\n",
    "            production_clean.isnull().sum().sum(),\n",
    "            production_clean.duplicated().sum(),\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Zeilen\", \"Fehlende Werte\", \"Duplikate\"],\n",
    ")\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Visualisierung der Bereinigung\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"🧹 Datenbereinigung: Vorher vs. Nachher\", fontsize=16)\n",
    "\n",
    "# Beispiel-Visualisierung mit Maschine A\n",
    "col_prod = \"Maschine_A_Stück\"\n",
    "col_avail = \"Verfügbarkeit_A_%\"\n",
    "\n",
    "# Vorher - Produktion\n",
    "axes[0, 0].hist(\n",
    "    production_df[col_prod], bins=30, alpha=0.7, color=\"red\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0, 0].set_title(f\"Vorher: {col_prod}\")\n",
    "axes[0, 0].set_xlabel(\"Stück pro Tag\")\n",
    "axes[0, 0].set_ylabel(\"Häufigkeit\")\n",
    "\n",
    "# Nachher - Produktion\n",
    "axes[0, 1].hist(\n",
    "    production_clean[col_prod], bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0, 1].set_title(f\"Nachher: {col_prod}\")\n",
    "axes[0, 1].set_xlabel(\"Stück pro Tag\")\n",
    "axes[0, 1].set_ylabel(\"Häufigkeit\")\n",
    "\n",
    "# Vorher - Verfügbarkeit\n",
    "axes[1, 0].hist(\n",
    "    production_df[col_avail], bins=30, alpha=0.7, color=\"red\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 0].set_title(f\"Vorher: {col_avail}\")\n",
    "axes[1, 0].set_xlabel(\"Verfügbarkeit %\")\n",
    "axes[1, 0].set_ylabel(\"Häufigkeit\")\n",
    "\n",
    "# Nachher - Verfügbarkeit\n",
    "axes[1, 1].hist(\n",
    "    production_clean[col_avail], bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 1].set_title(f\"Nachher: {col_avail}\")\n",
    "axes[1, 1].set_xlabel(\"Verfügbarkeit %\")\n",
    "axes[1, 1].set_ylabel(\"Häufigkeit\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Export und Speicherung {#export}\n",
    "\n",
    "### 📝 Daten in verschiedenen Formaten exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export-Funktionen\n",
    "def export_data_comprehensive(df, base_filename, include_formats=None):\n",
    "    \"\"\"\n",
    "    Exportiert DataFrame in verschiedene Formate\n",
    "    \"\"\"\n",
    "    if include_formats is None:\n",
    "        include_formats = [\"csv\", \"excel\", \"json\", \"parquet\"]\n",
    "\n",
    "    print(f\"💾 Exportiere {df.shape[0]} Zeilen in {len(include_formats)} Formaten...\")\n",
    "    exported_files = []\n",
    "\n",
    "    # CSV Export\n",
    "    if \"csv\" in include_formats:\n",
    "        csv_file = f\"../../data/{base_filename}.csv\"\n",
    "        df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "        file_size = Path(csv_file).stat().st_size / 1024\n",
    "        exported_files.append(\n",
    "            {\"Format\": \"CSV\", \"Datei\": csv_file, \"Größe_KB\": f\"{file_size:.1f}\"}\n",
    "        )\n",
    "        print(f\"✅ CSV: {csv_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "    # Excel Export\n",
    "    if \"excel\" in include_formats:\n",
    "        excel_file = f\"../../data/{base_filename}.xlsx\"\n",
    "        with pd.ExcelWriter(excel_file, engine=\"openpyxl\") as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Daten\", index=False)\n",
    "\n",
    "            # Zusätzliches Statistik-Blatt\n",
    "            if not df.select_dtypes(include=[np.number]).empty:\n",
    "                stats = df.select_dtypes(include=[np.number]).describe()\n",
    "                stats.to_excel(writer, sheet_name=\"Statistiken\")\n",
    "\n",
    "        file_size = Path(excel_file).stat().st_size / 1024\n",
    "        exported_files.append(\n",
    "            {\"Format\": \"Excel\", \"Datei\": excel_file, \"Größe_KB\": f\"{file_size:.1f}\"}\n",
    "        )\n",
    "        print(f\"✅ Excel: {excel_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "    # JSON Export\n",
    "    if \"json\" in include_formats:\n",
    "        json_file = f\"../../data/{base_filename}.json\"\n",
    "        # DataFrame zu Records konvertieren für bessere JSON-Struktur\n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                \"exported_at\": pd.Timestamp.now().isoformat(),\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": len(df.columns),\n",
    "                \"source\": \"Bystronic Data Pipeline\",\n",
    "            },\n",
    "            \"data\": df.to_dict(\"records\"),\n",
    "        }\n",
    "\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, default=str, ensure_ascii=False)\n",
    "\n",
    "        file_size = Path(json_file).stat().st_size / 1024\n",
    "        exported_files.append(\n",
    "            {\"Format\": \"JSON\", \"Datei\": json_file, \"Größe_KB\": f\"{file_size:.1f}\"}\n",
    "        )\n",
    "        print(f\"✅ JSON: {json_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "    # Parquet Export (effizienter für große Datensätze)\n",
    "    if \"parquet\" in include_formats:\n",
    "        try:\n",
    "            parquet_file = f\"../../data/{base_filename}.parquet\"\n",
    "            df.to_parquet(parquet_file, index=False, compression=\"snappy\")\n",
    "            file_size = Path(parquet_file).stat().st_size / 1024\n",
    "            exported_files.append(\n",
    "                {\n",
    "                    \"Format\": \"Parquet\",\n",
    "                    \"Datei\": parquet_file,\n",
    "                    \"Größe_KB\": f\"{file_size:.1f}\",\n",
    "                }\n",
    "            )\n",
    "            print(f\"✅ Parquet: {parquet_file} ({file_size:.1f} KB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Parquet Export fehlgeschlagen: {e}\")\n",
    "\n",
    "    # Zusammenfassung\n",
    "    summary_df = pd.DataFrame(exported_files)\n",
    "    print(\"\\n📋 Export-Zusammenfassung:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    return exported_files\n",
    "\n",
    "\n",
    "# Bereinigte Produktionsdaten exportieren\n",
    "production_exports = export_data_comprehensive(\n",
    "    production_clean, \"bystronic_production_clean\"\n",
    ")\n",
    "\n",
    "# Sensordaten exportieren\n",
    "sensor_exports = export_data_comprehensive(\n",
    "    df_sensors, \"bystronic_sensors\", [\"csv\", \"json\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n🎉 Gesamt exportiert: {len(production_exports) + len(sensor_exports)} Dateien\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Praxis-Pipeline {#pipeline}\n",
    "\n",
    "### 📝 Vollständige Datenverarbeitungs-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vollständige Datenverarbeitungs-Pipeline\n",
    "class BystronicDataPipeline:\n",
    "    \"\"\"\n",
    "    Vollständige Datenverarbeitungs-Pipeline für Bystronic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or self._default_config()\n",
    "        self.processed_data = {}\n",
    "        self.metadata = {}\n",
    "        self.log = []\n",
    "\n",
    "    def _default_config(self):\n",
    "        return {\n",
    "            \"csv_delimiter\": \"\\t\",\n",
    "            \"csv_encoding\": \"utf-8\",\n",
    "            \"clean_data\": True,\n",
    "            \"export_formats\": [\"csv\", \"excel\", \"json\"],\n",
    "            \"output_dir\": \"../../data/pipeline_output/\",\n",
    "        }\n",
    "\n",
    "    def log_message(self, message, level=\"INFO\"):\n",
    "        \"\"\"Logging für Pipeline\"\"\"\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {level}: {message}\"\n",
    "        self.log.append(log_entry)\n",
    "        print(log_entry)\n",
    "\n",
    "    def import_csv(self, file_path, data_name):\n",
    "        \"\"\"CSV Import mit intelligenter Struktur-Erkennung\"\"\"\n",
    "        self.log_message(f\"Importiere CSV: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            result = parse_bystronic_csv(\n",
    "                file_path,\n",
    "                delimiter=self.config[\"csv_delimiter\"],\n",
    "                encoding=self.config[\"csv_encoding\"],\n",
    "            )\n",
    "\n",
    "            self.processed_data[data_name] = result[\"data\"]\n",
    "            self.metadata[data_name] = result[\"metadata\"]\n",
    "\n",
    "            self.log_message(f\"CSV Import erfolgreich: {result['data'].shape}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"CSV Import fehlgeschlagen: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def import_excel(self, file_path, data_name):\n",
    "        \"\"\"Excel Import\"\"\"\n",
    "        self.log_message(f\"Importiere Excel: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "            for sheet_name, df in excel_data.items():\n",
    "                full_name = f\"{data_name}_{sheet_name}\"\n",
    "                self.processed_data[full_name] = df\n",
    "                self.log_message(f\"Arbeitsblatt '{sheet_name}': {df.shape}\")\n",
    "\n",
    "            self.log_message(\n",
    "                f\"Excel Import erfolgreich: {len(excel_data)} Arbeitsblätter\"\n",
    "            )\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"Excel Import fehlgeschlagen: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def import_api(self, api_type, data_name):\n",
    "        \"\"\"API Import (simuliert)\"\"\"\n",
    "        self.log_message(f\"Importiere API-Daten: {api_type}\")\n",
    "\n",
    "        try:\n",
    "            api_data = simulate_api_call(api_type)\n",
    "\n",
    "            if api_data[\"status\"] == \"success\":\n",
    "                if api_type == \"production\":\n",
    "                    df = pd.json_normalize(api_data[\"data\"][\"machines\"])\n",
    "                elif api_type == \"quality\":\n",
    "                    df = pd.DataFrame([api_data[\"data\"][\"daily_summary\"]])\n",
    "                else:\n",
    "                    df = pd.DataFrame([api_data[\"data\"]])\n",
    "\n",
    "                self.processed_data[data_name] = df\n",
    "                self.log_message(f\"API Import erfolgreich: {df.shape}\")\n",
    "                return True\n",
    "            else:\n",
    "                self.log_message(\n",
    "                    f\"API Fehler: {api_data.get('message', 'Unknown error')}\", \"ERROR\"\n",
    "                )\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_message(f\"API Import fehlgeschlagen: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def clean_all_data(self):\n",
    "        \"\"\"Bereinigt alle importierten Daten\"\"\"\n",
    "        if not self.config[\"clean_data\"]:\n",
    "            self.log_message(\"Datenbereinigung deaktiviert\")\n",
    "            return\n",
    "\n",
    "        self.log_message(\"Starte Datenbereinigung für alle Datasets\")\n",
    "\n",
    "        for data_name, df in self.processed_data.items():\n",
    "            try:\n",
    "                # Grundlegende Bereinigung\n",
    "                initial_rows = len(df)\n",
    "\n",
    "                # Duplikate entfernen\n",
    "                df_clean = df.drop_duplicates()\n",
    "\n",
    "                # Leere Spalten entfernen\n",
    "                df_clean = df_clean.dropna(axis=1, how=\"all\")\n",
    "\n",
    "                # Komplett leere Zeilen entfernen\n",
    "                df_clean = df_clean.dropna(axis=0, how=\"all\")\n",
    "\n",
    "                final_rows = len(df_clean)\n",
    "                removed = initial_rows - final_rows\n",
    "\n",
    "                self.processed_data[data_name] = df_clean\n",
    "                self.log_message(\n",
    "                    f\"{data_name}: {removed} Zeilen entfernt, {final_rows} verbleiben\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_message(\n",
    "                    f\"Bereinigung von {data_name} fehlgeschlagen: {e}\", \"ERROR\"\n",
    "                )\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Erstellt einen Zusammenfassungsbericht\"\"\"\n",
    "        self.log_message(\"Generiere Pipeline-Report\")\n",
    "\n",
    "        report = {\n",
    "            \"pipeline_info\": {\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_datasets\": len(self.processed_data),\n",
    "                \"config\": self.config,\n",
    "            },\n",
    "            \"datasets\": {},\n",
    "            \"log\": self.log,\n",
    "        }\n",
    "\n",
    "        total_rows = 0\n",
    "        total_columns = 0\n",
    "\n",
    "        for data_name, df in self.processed_data.items():\n",
    "            dataset_info = {\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": len(df.columns),\n",
    "                \"memory_mb\": df.memory_usage(deep=True).sum() / 1024**2,\n",
    "                \"missing_values\": df.isnull().sum().sum(),\n",
    "                \"dtypes\": df.dtypes.value_counts().to_dict(),\n",
    "            }\n",
    "\n",
    "            report[\"datasets\"][data_name] = dataset_info\n",
    "            total_rows += dataset_info[\"rows\"]\n",
    "            total_columns += dataset_info[\"columns\"]\n",
    "\n",
    "        report[\"pipeline_info\"][\"total_rows\"] = total_rows\n",
    "        report[\"pipeline_info\"][\"total_columns\"] = total_columns\n",
    "\n",
    "        return report\n",
    "\n",
    "    def export_all(self, include_report=True):\n",
    "        \"\"\"Exportiert alle verarbeiteten Daten\"\"\"\n",
    "        self.log_message(\"Starte Export aller Daten\")\n",
    "\n",
    "        # Output-Verzeichnis erstellen\n",
    "        output_dir = Path(self.config[\"output_dir\"])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        exported_files = []\n",
    "\n",
    "        for data_name, df in self.processed_data.items():\n",
    "            try:\n",
    "                file_path = output_dir / f\"{data_name}_processed\"\n",
    "\n",
    "                # CSV Export\n",
    "                if \"csv\" in self.config[\"export_formats\"]:\n",
    "                    csv_file = f\"{file_path}.csv\"\n",
    "                    df.to_csv(csv_file, index=False)\n",
    "                    exported_files.append(csv_file)\n",
    "\n",
    "                # Excel Export\n",
    "                if \"excel\" in self.config[\"export_formats\"]:\n",
    "                    excel_file = f\"{file_path}.xlsx\"\n",
    "                    df.to_excel(excel_file, index=False)\n",
    "                    exported_files.append(excel_file)\n",
    "\n",
    "                # JSON Export\n",
    "                if \"json\" in self.config[\"export_formats\"]:\n",
    "                    json_file = f\"{file_path}.json\"\n",
    "                    df.to_json(json_file, orient=\"records\", indent=2)\n",
    "                    exported_files.append(json_file)\n",
    "\n",
    "                self.log_message(f\"Export für {data_name} abgeschlossen\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_message(f\"Export von {data_name} fehlgeschlagen: {e}\", \"ERROR\")\n",
    "\n",
    "        # Pipeline-Report exportieren\n",
    "        if include_report:\n",
    "            report = self.generate_report()\n",
    "            report_file = output_dir / \"pipeline_report.json\"\n",
    "\n",
    "            with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(report, f, indent=2, default=str, ensure_ascii=False)\n",
    "\n",
    "            exported_files.append(str(report_file))\n",
    "            self.log_message(f\"Pipeline-Report: {report_file}\")\n",
    "\n",
    "        self.log_message(f\"Export abgeschlossen: {len(exported_files)} Dateien\")\n",
    "        return exported_files\n",
    "\n",
    "\n",
    "# Pipeline testen\n",
    "pipeline = BystronicDataPipeline()\n",
    "\n",
    "print(\"🚀 Starte Bystronic Datenverarbeitungs-Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daten importieren\n",
    "pipeline.import_excel(excel_file, \"production\")\n",
    "pipeline.import_api(\"production\", \"live_machines\")\n",
    "pipeline.import_api(\"quality\", \"quality_metrics\")\n",
    "\n",
    "# Daten bereinigen\n",
    "pipeline.clean_all_data()\n",
    "\n",
    "# Report generieren\n",
    "report = pipeline.generate_report()\n",
    "\n",
    "print(\"\\n📊 Pipeline Zusammenfassung:\")\n",
    "print(f\"📂 Datasets: {report['pipeline_info']['total_datasets']}\")\n",
    "print(f\"📈 Gesamt Zeilen: {report['pipeline_info']['total_rows']:,}\")\n",
    "print(f\"🔢 Gesamt Spalten: {report['pipeline_info']['total_columns']:,}\")\n",
    "\n",
    "for dataset_name, info in report[\"datasets\"].items():\n",
    "    print(f\"  • {dataset_name}: {info['rows']:,} Zeilen × {info['columns']} Spalten\")\n",
    "\n",
    "# Exportieren\n",
    "exported_files = pipeline.export_all()\n",
    "\n",
    "print(f\"\\n✅ Pipeline abgeschlossen! {len(exported_files)} Dateien exportiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 🎯 Zusammenfassung\n\n### Was Sie gelernt haben:\n\n1. **CSV-Import mit komplexen Strukturen**\n   - Intelligente Header-Erkennung\n   - Flexible Trennzeichen-Unterstützung\n   - Metadaten-Extraktion\n\n2. **Excel-Verarbeitung**\n   - Mehrere Arbeitsblätter gleichzeitig laden\n   - Strukturierte Datenorganisation\n   - Format-spezifische Features nutzen\n\n3. **JSON und API-Integration**\n   - Hierarchische Datenstrukturen verarbeiten\n   - API-Simulation und echte Webservice-Aufrufe\n   - JSON-Normalisierung für tabellarische Daten\n\n4. **Datenbereinigung**\n   - Datenqualität systematisch analysieren\n   - Ausreißer und Anomalien erkennen\n   - Automatische Bereinigungsregeln implementieren\n\n5. **Export-Strategien**\n   - Mehrere Formate gleichzeitig\n   - Dateigröße und Performance berücksichtigen\n   - Metadaten und Dokumentation einschließen\n\n6. **Produktions-Pipeline**\n   - Vollständige Automatisierung\n   - Fehlerbehandlung und Logging\n   - Konfigurierbare Verarbeitungsschritte\n\n### 💡 Tipps für die Praxis:\n\n- **Immer Daten validieren** bevor Verarbeitung\n- **Encoding explizit definieren** (UTF-8, Latin1, etc.)\n- **Chunk-Processing** für sehr große Dateien\n- **Fehlerbehandlung** für robuste Pipelines\n- **Dokumentation** der Datenquellen und -transformationen\n\n### 🔗 Nächste Schritte:\n\n- **Modul 07**: Jupyter Notebooks für interaktive Analysen\n- **Modul 08**: Benutzeroberflächen mit PyQt/PySide und Streamlit\n- **Modul 09**: Vollständige Praxisprojekte\n\n---\n\n*Herzlichen Glückwunsch! Sie beherrschen jetzt die wichtigsten Techniken für Datenimport und -verarbeitung in Python für industrielle Anwendungen.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bystronic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
